#### 网络模型

* 阻塞IO：顾名思义，阻塞lO就是两个阶段都必须阻寒等待:

* ![1721920057542](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721920057542.png)
  
* 可以看到，阻塞IO模型中，用户进程在两个阶段都是阻塞状态。
  
* 非阻塞IO:顾名思义，非阻塞IO的recvfrom操作会立即返回结果而不是阻塞用户进程。

  ![1721920915663](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721920915663.png)
  
  * 可以看到，非阻塞IO模型中，用户进程在第一个阶段是非阻塞，第二个阶段是阻塞状态。虽然是非阻塞，但性能并没有得到提高。而且忙等机制会导致CPU空转，CPU使用率暴增。



* IO多路复用

  * 无论是阻塞IO还是非阻塞lO，用户应用在一阶段都需要调用recvfrom来获取数据，差别在于无数据时的处理方案:

    * 如果调用recvfrom时，恰好没有数据，阻塞IO会使进程阻塞，非阻塞lO使CPU空转，都不能充分发挥CPU的作用。

    * 如果调用recvfrom时，恰好有数据，则用户进程可以直接进入第二阶段，读取并处理数据

    * 比如服务端处理客户端Socket请求时，在单线程情况下，只能依次处理每一个socket，如果正在处理的socket恰好未就绪（数据不可读或不可写)，线程就会被阻塞，所有其它客户端socket都必须等待，性能自然会很差。

    * 这就像服务员给顾客点餐，分两步:

      * 顾客思考要吃什么（等待数据就绪)

      * 顾客想好了，开始点餐（读取数据)

        ![1721921540968](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721921540968.png)

    * 要提高效率有几种办法?

      * 方案一:增加更多服务员〔多线程)
      * 方案二:不排队，谁想好了吃什么（数据就绪了)，服务员就给谁点餐（用户应用就去读取数据)

    * 文件描述符（File Descriptor):简称FD，是一个从O开始递增的无符号整数，用来关联Linux中的一个文件。在Linux中，一切皆文件，例如常规文件、视频、硬件设备等，当然也包括网络套接字(Socket)。

    * l0多路复用:是利用单个线程来同时监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。

      ![1721921845076](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721921845076.png)

      * select可以监听多个FD，可以放多个进去，recvfrom只能一个FD

  * I0多路复用:是利用单个线程来同时监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。

  * 不过监听FD的方式、通知的方式又有多种实现,常见的有:

    * select

    * poll

      ![1721922179661](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721922179661.png)

    * epoll

    * **差异：select和poll只会通知用户进程有FD就绪。但不确定具体是哪个FD，需要用户进程逐个遍历FD来确认epoll则会在通知用户进程FD就绪的同时，把已就绪的FD写入用户空间**
    
  * select是Linux最早使用的I/O多路复用方案
  
    ![1721958546444](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721958546444.png)
  
    * fds_bits数组是32个bit，因为__FD_SETSIZE  /__NFDBITS是32，而fds_bits是long int类型。是4个字节，也是32bit，所以总共能表示32*32=1024
  
    * 他首先会创建一个类似于位示图，需要从用户态转入到内核态
  
      ![1721960502591](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721960502591.png)
  
    * 内核空间会通过遍历这个序列，获得请求的1，然后把就绪的1还是1，未就绪的改为0。因为用户空间并不知道就绪，所以需要把这个序列再传入到用户空间，覆盖原来的，此时保留的就是就绪的那些FD
  
      ![1721960702683](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721960702683.png)
  
    * 用户空间再遍历这个序列获取哪个就绪
  
    * 问题：
  
      * 需要将整个fd_set从用户空间拷贝到内核空间,select结束还要再次拷贝回用户空间
      * select无法得知具体是哪个fd就绪，需要遍历整个fd_set
      * fd_set监听的fd数量不能超过1024
  
  * poll模式
  
    * poll模式对select模式做了简单改进，但性能提升不明显，部分关键代码如下:
  
      ![1721961353786](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721961353786.png)
  
    * poll执行流程如下：
  
      * 创建pollfd数组，向其中添加关注的fd信息，数组大小自定义
      * 调用poll函数，将pollfd数组拷贝到内核空间，转链表存储，无上限
      * 内核遍历fd,判断是否就绪
      * 数据就绪或超时后，拷贝pollfd数组到用户空间，返回就绪fd数量n
      * 用户进程判断n是否大于0
      * 大于0则遍历pollfd数组，找到就绪的fd
  
    * 与select对比:
  
      *  select模式中的fd_set大小固定为1024，而pollfd在内核中采用
        链表，理论上无上限
      * 监听FD越多，每次遍历消耗时间也越久，性能反而会下降
  
  * epoll
  
    * epoll模式是对select和poll的改进，它提供了三个函数:
  
      ![1721977967441](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721977967441.png)
  
    * 就绪的放到list_head中，然后拷贝到用户空间的events，他和select区别是，他只拷贝了就绪PD。
  
      ![1721978565865](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721978565865.png)
  
  * 总结
  
    * select模式存在的三个问题:
      * 能监听的FD最大不超过1024
      * 每次select都需要把所有要监听的FD都拷贝到内核空间
      * 每次都要遍历所有FD来判断就绪状态
    * poll模式的问题:
      * poll利用链表解决了select中监听FD上限的问题，但依然要遍历所有
      * FD，如果监听较多，性能会下降
    * epoll模式中如何解决这些问题的?
      * 基于epoll实例中的红黑树保存要监听的FD，理论上无上限，而且增
        删改查效率都非常高，性能不会随监听的FD数量增多而下降
      * 每个FD只需要执行一次epoll_ctl添加到红黑树，以后每次epol_wait无需传递任何参数，无需重复拷贝FD到内核空间
      * 内核会将就绪的FD直接拷贝到用户空间的指定位置，用户进程无需遍历所有FD就能知道就绪的FD是谁
  
* IO多路复用—–事件通知机制

  * 当FD有数据可读时，我们调用epoll_wait就可以得到通知。但是事件通知的模式有两种:
    * LevelTriggered:简称LT。当FD有数据可读时，会重复通知多次，直至数据处理完成。是Epoll的默认模式。
    * EdgeTriggered:简称ET。当FD有数据可读时，只会被通知一次，不管数据是否处理完成。
  * ET模式避免了LT模式可能出现的惊群现象
  * ET模式最好结合非阻塞IO读取FD数据，相比LT会复杂一些



* IO多路复用—-web服务流程

  * 基于epoll模式的web服务的基本流程如图：

    ![1721982983754](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1721982983754.png)






* 信号驱动IO

  * 信号驱动I0是与内核建立SIGIO的信号关联并设置回调，当内核有FD就绪时，会发出SIGIO信号通知用户，期间用户应用可以执行其它业务，无需阻塞等待。

    ![1722047735404](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1722047735404.png)

  * 当有大量I0操作时，信号较多，SIGIO处理函数不能及时处理可能导致信号队列溢出而且内核空间与用户空间的频繁信号交互性能也较低。

* 异步IO

  * 用户进程不用阻塞。异步lO的整个过程都是非阻塞的，用户进程调用完异步API后就可以去做其它事情，内核等待数据就绪并拷贝到用户空间后才会递交信号，通知用户进程。

    ![1722047890882](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1722047890882.png)

  * 耗费内存空间

* I0操作是同步还是异步，关键看数据在内核空间与用户空间的拷贝过程（数据读写的IO操作），也就是阶段二是同步还是异步:

  ![1722048333503](redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/1722048333503.png)







##### REDIS网络模型

* Redis到底是单线程还是多线程?
  * 如果仅仅聊Redis的核心业务部分（命令处理)，答案是单线程
  * 如果是聊整个Redis，那么答案就是多线程
* 在Redis版本迭代过程中，在两个重要的时间节点上引入了多线程的支持:
  * Redis v4.0:引入多线程异步处理一些耗时较长的任务，例如异步删除命令unlink
  * Redis v6.0:在核心网络模型中引入多线程，进一步提高对于多核CPU的利用率
* 1．为什么Redis要选择单线程?
  * 抛开持久化不谈，Redis是纯内存操作，执行速度非常快，它的牲能瓶颈是网络延迟而不是执行速度，因此多线程并不会带来巨大的性能提升。
  * 多线程会导致过多的上下文切换，带来不必要的开销
  * 引入多线程会面临线程安全问题，必然要引入线程锁。这样的安全手段，实现复杂度增高，而且性能也会大打折扣