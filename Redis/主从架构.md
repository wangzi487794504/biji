#### 主从架构

* 单节点Redis的并发能力是有上限的，要进一步提高Redis的并发能力，就需要搭建主从集群，实现读写分离。（因为redis读多写少）

* 开启主从关系

  * 要配置主从可以使用replicaof或者slaveof(5.0以前)命令。
  * 有永久和临时两种
    * 修改配置文件(永久生效)
      * 在redis.conf中添加一行配置:slaveof  <masterip> <masterport>
      * 使用redis-cli客户端连接到redis服务，执行slaveof命令(重启后失效)∶
        slaveof  <masterip><masterport>
    * infoapplication查看状态信息

* 数据同步原理

  * 主从第一次同步是全量同步:（先使用bgsave生成RDB文件，再使用repl_baklog保存RDB文件之后的数据，所以先使用RDB，再使用repl_baklog）

    ![1721550337550](%E4%B8%BB%E4%BB%8E%E6%9E%B6%E6%9E%84.assets/1721550337550.png)

  * master如何判断slave是不是第一次来同步数据?这里会用到两个很重要的概念:

    *  Replication ld:简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid
    * offset:偏移量随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset,。如果slave的offset小于master的offset，"说明slave数据落后于master，需要更新。

  * 因此slave做数据同步，必须向master声明自己的replication id和offset,master才可以判断到底需要同步哪些数据

  * ==简述全量同步的流程?. slave节点请求增量同步==

    * master节点判断replid，发现不一致，拒绝增量同步
    * master将完整内存数据生成RDB，发送RDB到slave
    * slave清空本地数据，加载master的RDB
    * master将RDB期间的命令记录在repl_baklog，并持续将log中的命令发送给slave
    * slave执行接收到的命令，保持与master之间的同步

* 增量同步

  * 主从第一次同步是全量同步，但如果slave重启后同步，则执行增量同步

    ![1721550967390](%E4%B8%BB%E4%BB%8E%E6%9E%B6%E6%9E%84.assets/1721550967390.png)

    * ==offset是一个循环队列，只要主节点的长度没有比从节点的长度超过一圈，都可以用增量同步，超过了就得用全量同步==

* ==可以从以下几个方面来优化Redis主从就集群:==

  * 在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘IO。（这种就是直接网络传输。要求网络带宽比较大）

  * Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘IO

  * 适当提高repl_baklog的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步

  * 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力

    ![1721551577765](%E4%B8%BB%E4%BB%8E%E6%9E%B6%E6%9E%84.assets/1721551577765.png)





##### 哨兵模式

* Redis提供了哨兵（Sentinel)机制来实现主从集群的自动故障恢复。

  * 监控:Sentinel会不断检查您的master和slave是否按预期工作
  * 自动故障恢复:如果master故障，Sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主
  * 通知: Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移时，会将最新信息推送给Redis的客户端

* 服务状态监控

  * Sentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令:
    * 主观下线:如果某sentinel节点发现某实例未在规定时间响应，则认为该实例主观下线
    * 客观下线︰==若超过指定数量（quorum)的sentinel都认为该实例主观下线，则该实例客观下线。quorum值最好超过Sentinel实例数量的一半。==

* 选举新的master

  * 一旦发现master故障，sentinel需要在salve中选择一个作为新的master，选择依据是这样的:
    * 首先会判断slave节点与master节点断开时间长短，如果超过指定值（down-after-milliseconds*10)则会排除该slave节点
    * 然后判断slave节点的slave-priority值，越小优先级越高，如果是O则永不参与选举
    * 如果slave-prority一样，则判断slave节点的offset值，越大说明数据越新，优先级越高
    * 最后是判断slave节点的运行id大小，越小优先级越高。

* 故障转移

  * 当选中了其中一个slave为新的master后（例如slave1)，故障的转移的步骤如下：

    * sentinel给备选的slave1节点发送slaveof no one命令，让该节点成为master

    * sentinel给所有其它slave发送slaveof 192.168.150.101 7002命令，让这些slave成为新master的从节点，开始从新的master上同步数据。

    * 最后，sentinel将故障节点标记为slave，当故障节点恢复后会自动成为新的master的slave节点

      <img src="%E4%B8%BB%E4%BB%8E%E6%9E%B6%E6%9E%84.assets/1721552854453.png" alt="1721552854453" style="zoom:67%;" />

