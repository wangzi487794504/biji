###Image fusion in the loop ofhigh-level vision tasks: A semantic-aware real-time infrared and visible image fusion network

**摘要：**红外与可见光图像融合是一种融合后的图像，==融合后的图像既包含显著目标和丰富的纹理细节，又便于实现高级视觉任务。==然而，现有的融合算法片面地关注融合图像的视觉质量和统计指标，而忽略了高级视觉任务的需求。为了解决这些问题，本文提出了一种基于语义感知的实时图像融合网络(SeAFusion)。一方面，将图像融合模块和语义分割模块级联，利用语义损失引导高级语义信息回流到图像融合模块，有效提高融合图像的高级视觉任务性能;另一方面，为了增强融合网络对细粒度空间细节的描述能力，我们设计了梯度残差稠密块(GRDB)。广泛的比较和泛化实验证明了我们的SeAFusion在保持像素强度分布和保持纹理细节方面的优越性。更重要的是，通过对各种融合算法在任务驱动评估中的性能比较，揭示了我们的框架在促进高级视觉任务方面的天然优势。此外，卓越的运行效率使我们的算法可以毫不费力地部署为高级视觉任务的实时预处理模块。源代码将在https://github.com/Linfeng-Tang/SeAFusion发布。

####一 .介绍

**由于理论和技术的限制，单模态传感器拍摄的图像不能有效、全面地描述成像场景[1]。红外传感器捕捉物体发出的热辐射，可以突出突出目标，但红外图像忽略了纹理，易受噪声影响。相反，可见光传感器捕捉反射光信息。可见图像通常包含丰富的纹理和结构信息，但对环境敏感，如光照和遮挡。**这种互补性的作用促使我们将红外图像和可见光图像融合在一起，生成理想的图像，突出突出的目标，并体现丰富的细节信息。因此，红外与可见光图像融合被广泛地用作高级视觉任务的预处理模块，如目标检测[2]、跟踪[3]、行人再识别[4]、语义分割[5]等。图1中的一个例子直观地展示了融合图像对分割任务的贡献。分割网络可以从可见图像中分割出汽车、自行车和几个人，而忽略隐藏在黑暗中的行人。虽然红外图像有助于准确分割网络分割车辆和行人，但忽略了自行车。融合后的图像利用红外和可见光图像的互补信息，提高了所有自行车、汽车和行人的分割精度。

由于红外与可见光图像融合的实用性，引起了学术界的广泛关注。在过去的几十年里，提出了大量的图像融合技术，包括传统的方法[8-10]和最近的基于深度学习的方法[1]。传统方法主要有基于多尺度变换(MST)的方法[11-14]、基于稀疏表示(SR)的方法[15,16]、基于子空间的方法[17 - 19]、基于优化的方法[20]和混合方法[21]。在基于深度学习的方法中，基于自编码器(auto-encoder, AE)框架[6,22,23]、基于卷积神经网络(convolutional neural network, CNN)框架[24-26]和基于生成式对抗网络(generative adversarial network, GAN)框架[27-30]是主流框架。

尽管近年来基于深度学习的图像融合算法能够生成令人满意的融合图像，但在图像融合领域仍存在一些紧迫的挑战。**一方面，现有的融合算法倾向于追求更好的视觉质量和更高的评价指标，而很少系统地考虑融合后的图像是否能够促进高水平的视觉任务。**==一些研究[31 - 33]表明，仅考虑视觉质量和定量指标对高水平视觉任务没有帮助。虽然有些文献在特征层面引入了感知损失来约束融合图像和源图像[6,7,24,28,34]，但感知损失并不能有效增强融合图像中的语义信息，如图1所示。==另外，也有研究者通过一个分割掩模来指导图像融合过程[26,35]，但该掩模只分割了一些显著目标，对语义信息的增强作用有限。另一方面，现有的评价方式主要是视觉比较和定量评价。视觉比较侧重于融合图像的对比度和纹理细节，定量评价依赖于一些统计指标来评价融合性能。然而，无论是视觉比较还是定量评价都不能反映融合图像对高级视觉任务的促进作用。此外，现有的网络体系结构不能有效地提取细粒度的细节特征。最后，现有的许多融合算法忽略了对实时图像融合的需求，而致力于提高视觉质量和评价指标。

本研究提出了一种语义感知融合网络，称为SeAFusion，以实现实时红外与可见光图像的融合。该方法的关键是在图像融合和高级视觉任务中都能获得良好的性能。具体来说，我们引入了一个分割网络来预测融合图像的分割结果，并利用该网络构建语义损失。然后，利用语义丢失来指导融合网络的反向传播训练，迫使融合图像包含更多的语义信息。此外，为了满足高级视觉任务的实时需求，我们开发了一种基于梯度残差稠密块(GRDB)的轻量级网络。该算法通过主密集流实现特征重用，通过残余梯度流提高对细粒度细节的描述能力。

综上所述，本研究的主要贡献如下:

* 我们设计了一种新的语义感知的红外和可见光图像融合框架，该框架在图像融合和高级视觉任务中都能有效地实现卓越的性能。
* 设计一个梯度残差稠密块，提高网络对细粒度细节的描述能力，实现特征重用。
* 所提出的SeAFusion是一个可以实现实时图像融合的轻量化模型。这允许它被部署为高级愿景任务的预处理模块。∙我们提出了一种任务驱动的评估方式，从高级视觉任务的角度评估图像融合的性能。

本文的其余部分组织如下。第二节简要介绍了图像融合和任务驱动算法的相关工作。在第三节中，我们详细介绍了我们的方案，包括问题分析、损耗功能、网络结构和培训策略。第4节说明了与其他替代方法相比，我们的方法的令人印象深刻的性能，随后在第5节中有一些结论性的评论。

#### 二. 相关工作

本节首先回顾了现有的红外和可见光图像融合算法。然后，简要介绍了一些任务驱动的低层次视觉算法。

##### 2.1 图像融合算法

###### 2.1.1 传统图像融合方法

由于特征重构通常是特征提取的逆过程，传统图像融合算法的关键在于特征提取和融合两个关键要素。多尺度分解是特征提取中最常用的一种变换方法。在过去的几十年里，拉普拉斯金字塔(LP)、离散小波[11]、shearlet[12]、非下采样contourlet[13]、潜在低秩表示[15]等多种多尺度变换已经成功嵌入到基于多尺度变换的图像融合框架中。此外，稀疏表示作为一种特征提取技术，利用过完备字典中的稀疏基来表示源图像[16,36]。此外，基于子空间的方法也受到了广泛的关注，这种方法将高维图像投影到低维的子空间中，以获取源图像的内在结构。独立分量分析[17]、主分量分析[19]、非负矩阵分解[18]是基于子空间的融合框架中的代表性方法。

除上述方法外，基于优化的方法为图像融合领域提供了新的视角和前景。特别是Ma等人将红外与可见光图像融合定义为整体强度保持和纹理结构保持，这为基于cnn的方法[26]和基于gan的方法[20]奠定了坚实的基础。此外，也有研究者结合不同框架的优点，提出混合模型来追求更好的图像融合性能[21,37]。特别是Liu等人将多尺度变换(MST)和稀疏表示(SR)相结合，开发了一种通用的图像融合框架，同时克服了基于MST和基于SR的融合方法的固有缺陷。

值得注意的是，日益复杂的变换或表示不能响应实时图像融合[38]的要求。此外，手工制作的活动水平测量和融合规则的活动水平测量不能集成语义信息，这将限制融合结果对高级视觉任务的贡献。

###### 2.1.2 基于AE的图像融合方法

由于神经网络优越的特征学习能力，深度学习已成为众多视觉任务的新宠。图像融合社区也在积极探索基于深度学习的解决方案，并开发了许多有前途的方案。基于自动编码器(AE)的框架是一个重要的分支，它训练自动编码器来实现特征提取和重构。Li等人提出了一种简单的融合架构，该架构由编码器层、融合层和解码器层[22]三部分组成。编码器层包含一个卷积层和高级特征的密集块，其中密集块在编码过程中被用来获得更有用的特征。融合层利用元素加法策略或l1-norm策略对高级特征进行融合，特征重建网络包含四个卷积层对融合后的图像进行重建。此外，他们还引入了多尺度的编码器-解码器架构和嵌套连接来提取更全面的特征[6,23]。然而，上述方法都采用手工制作的融合规则来整合深度特征，严重限制了融合性能。为了解决人工设计融合规则的局限性，Xu等人提出了基于分类显著性的ae图像融合框架[39]规则。该融合规则利用分类器度量特征图中每个像素的显著性，并根据每个像素的贡献计算融合权值。

##### 2.1.3  基于cnn的图像融合方法

基于CNN的融合框架可以通过精细的丢失函数来实现隐式特征提取、聚合和图像重建，也可以将卷积神经网络(CNN)作为整体融合框架的一部分来实现活动水平测量和特征整合。LP-CNN是将CNN应用于图像融合领域的先驱，将LP与分类式CNN结合，实现医学图像融合[40]。此外，Zhang等人通过通用网络结构开发了一个通用的图像融合框架，即特征提取层、融合层和图像重建层[24]。值得注意的是，他们的融合层嵌入在训练过程中。因此，IFCNN可以减轻人工设计的融合规则(基于元素的max、基于元素的min或基于元素的均值)所施加的限制。

此外，研究人员还探索了另一种解决方案，即端到端基于cnn的图像融合框架，以避免手工规则的缺点。基于cnn的方法继承了传统的基于优化的方法的核心概念，将图像融合的目标函数定义为整体强度保真度和纹理结构保持[20]。Zhang等人将统一的图像融合建模为梯度和强度的比例维持，并为不同的图像融合任务设计了一个通用的损耗函数[41]。基于梯度路径和强度路径，他们还设计了一个压缩分解网络来提高融合图像[7]的保真度。此外，引入自适应决策块，根据源图像的纹理丰富度分配梯度损失项的权值。考虑到不同图像融合任务之间的交叉受精，Xu等人训练了一个用于多融合任务的统一模型[42]。为了增强融合图像中的语义信息，Ma et al.[26]利用显著掩模构造红外与可见光图像融合所需的信息。虽然所提出的网络能够检测出显著目标，但简单的显著目标掩码只是增强了显著目标区域的语义信息。此外，对于图像融合任务，很难提供真实的地面信息来构建损耗函数，这意味着基于cnn的融合网络无法充分释放其潜在性能。

###### 2.1.4   基于网络的图像融合方法

由于对抗损失是从概率分布的角度构建网络的，因此生成式对抗网络非常适合于无监督任务，如图像到图像的翻译[43,44]和图像融合[27,45]。Ma等人创造性地将GAN引入到图像融合社区中，利用鉴别器迫使生成器合成纹理丰富的融合图像[27]。为了提高细节信息的质量和锐化热目标的边缘，他们还引入了细节损失和边缘增强损失[28]。但是，单一的鉴别器可能会导致模式崩溃，即融合后的图像会偏向于可见光或红外图像。因此，Ma等人进一步提出了一种双鉴别器条件生成对抗网络，以提高基于gan的框架的鲁棒性，并保持红外图像和可见光图像[29]之间的平衡。随后，Li等人将多尺度注意机制整合到基于gan的融合框架[46]中，促使生成器和鉴别器更加关注典型区域。另外，Ma等人将图像融合转化为多分布同时估计问题，从分类器[47]的角度实现了红外图像与可见光图像的平衡。

然而，传统方法和基于深度学习的方法都强调了融合图像质量和评价指标的改进，而忽略了高水平视觉任务的需求。在实际应用中，具有良好图像质量的融合图像可能适合于人类的视觉感知，但可能不利于实现高水平的视觉任务。一种鲁棒的图像融合算法应在增强融合图像语义信息的同时充分整合源图像中的互补信息。一些基于深度学习的算法试图通过引入感知丢失或显著目标掩模来增强融合图像中的语义信息。但感知缺失对语义信息增强的作用有限。此外，显著目标掩模只强化了显著目标区域的语义信息。为此，开发一种语义感知的图像融合算法势在必行。

#####2.2 任务驱动的低层次视觉算法

事实上，结合低层算法和高层视觉任务的需求，已经提出了一些实用的解决方案。Li等人设计了一种轻量级的去雾网络，称为AOD-Net，它可以很容易地嵌入到其他深层模型中，例如Faster RCNN，用于改善雾霾图像[48]上的高级视觉任务。AODNet是第一个探索去雾算法与高级视觉任务性能之间相关性的工作。随后，Haris等人研究了图像超分辨率如何在低分辨率图像[31]中对目标进行检测。他们开发了一种新的超分辨率框架，其中超分辨率子网络明确地在其训练目标中包含检测损失。此外，Liu等人提出了一种任务驱动的图像去噪方案，该方案将图像去噪的两个模块与各种高级视觉任务级联，并使用联合损耗来仅更新去噪模型的参数[49,50]。他们的解决方案可以克服高级视觉任务的性能下降，并在高级视觉信息的引导下产生更具有视觉吸引力的结果。最近，郭等人考虑到自动驾驶的实际需求，设计了一种包含语义细化残差网络和两阶段分割感知联合训练策略[51]的新型脱轨框架。

本文设计了一种语义感知的实时红外与可见光图像融合框架，以增强融合图像中的语义信息。具体来说，我们首先引入语义损失，将更多的语义信息整合到融合图像中。然后，我们设计了一种低水平和高水平的联合自适应训练策略，以保持低水平和高水平视觉任务的平衡。最后，在语义丢失的引导下，该融合模型可以生成视觉上更吸引人的融合图像，并在高级视觉任务中获得良好的性能。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673842863636.png" alt="1673842863636" style="zoom:80%;" />



#### 三.方法论

在本节中，我们全面地描述了语义感知的实时红外和可见光图像融合框架。首先，我们提出了我国航海的问题规划。然后，详细介绍了内容损失和语义损失。然后，提出了基于梯度残差稠密块的融合网络结构。最后，详细介绍了培训策略。

##### 3.1  问题表述

给定一对配准红外图像𝐼𝑖𝑟∈R𝐻×𝑊×1和可见光图像𝐼𝑣𝑖∈R𝐻×𝑊×3，在定制损失函数的指导下，通过特征提取、聚合和重建实现图像融合。因此，融合后的图像𝐼𝑓∈R𝐻×𝑊×3的质量很大程度上依赖于损耗函数。为了提高融合性能，我们设计了一种包含内容损失和语义损失的联合损失来约束融合网络。我们的语义感知红外与可见光图像融合算法的总体框架如图2所示。

首先，设计了一种基于梯度残差稠密块(梯度残差稠密块，GRDB)的轻量化融合网络，充分整合源图像中的互补信息;具体来说，我们使用特征提取模块𝐸𝐹，从红外和可见光图像中提取具有丰富细粒度细节信息的深度特征，可以表示为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673842950695.png" alt="1673842950695" style="zoom:80%;" />

其中𝐹𝑖𝑟和𝐹𝑣𝑖分别表示红外特征和可见特征。此外，在特征提取模块中嵌入了grdb，在提取高级语义特征的同时提高了对细粒度细节的描述能力(我们将在第3.3节讨论其网络架构)。给定GRDB的输入𝐹𝑖，其输出𝐹𝑖+1可表示为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673842976570.png" alt="1673842976570" style="zoom:80%;" />

式中，𝐶𝑜𝑛𝑣(⋅)为卷积层，𝐶𝑜𝑛𝑣𝑛(⋅)为𝑛级联卷积层。∇是指梯度算子，即人工设计卷积核的一种特殊的卷积运算。梯度算子将输入特征与高频卷积核进行卷积，提取细粒度的细节信息。本研究利用著名的Sobel算子计算梯度幅值。⊕表示元素级求和。GRDB利用梯度幅度信息聚合可学习的卷积特征。

然后，通过特征融合和图像重建模块对融合后的图像进行重建。利用拼接融合策略，融合深红外和可见特征，包含丰富的细粒度空间细节。融合过程表示如下:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843010510.png" alt="1673843010510" style="zoom:80%;" />

其中C(⋅)为通道尺寸上的串联。最后，通过图像重构器𝐼从融合的特征𝐹𝑓得到融合后的图像𝐼𝑓，表示为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843048278.png" alt="1673843048278" style="zoom:80%;" />

此外，充分考虑高级视觉任务对融合图像的需求，采用语义损失来度量融合图像中包含的语义信息。具体来说，我们引入一个分割模型𝑁𝑠对融合后的图像𝐼𝑓∈R𝐻×𝑊×3[52]进行分割。分割结果𝐼𝑠∈R𝐻×𝑊×𝐶与语义标签𝐿𝑠∈(1，𝐶)𝐻×𝑊之间的差距能够反映融合图像中所包含语义信息的丰富程度，其中𝐻、𝑊分别为图像的高度、宽度，𝐶为对象类别的数量。假设融合图像𝐼𝑓，语义感知过程表示为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843073332.png" alt="1673843073332" style="zoom:80%;" />

分割结果与语义标签之间的差距记为<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843142994.png" alt="1673843142994" style="zoom:80%;" />，定义为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843213069.png" alt="1673843213069" style="zoom:80%;" />

其中![1673843249493](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843249493.png)为误差函数。关于误差函数的更多细节将在第3.2节中介绍。

##### 3.2 损失函数

我们的目标是增强融合图像的语义信息，同时提高视觉质量和评价指标。为了实现这些目标，我们从两个方面来设计我们的损失函数。一方面，需要充分整合源图像中的互补信息，如红外图像中的突出目标、可见光图像中的纹理细节等。为此，设计了内容损失，以保证融合图像的视觉逼真度。另一方面，融合后的图像能有效地促进高级视觉任务的完成。为此，我们构造了一种语义缺失来反映融合图像对高级视觉任务的贡献程度。

#####3.2.1  内容损失

为了促进我们的融合模型整合更有意义的信息，提高视觉质量和量化指标，我们设计了一个内容损失。含量损失由强度损失![1673843326062](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843326062.png)和质地损失![1673843339401](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843339401.png)两部分组成。内容损失的确切定义如下:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843444032.png" alt="1673843444032" style="zoom:80%;" />

其中![1673843476519](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843476519.png)约束融合图像的整体表观强度，![1673843492852](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843492852.png)强制融合图像包含更多的细粒度纹理细节。在这里，𝛼用于在强度损失![1673843519615](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843519615.png)和纹理损失![1673843531261](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673843531261.png)之间取得平衡。

强度损失是指融合后的图像与源图像在像素级上的差异。因此，我们将红外和可见光图像的强度损失定义为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673849811579.png" alt="1673849811579" style="zoom:80%;" />

 其中𝐻为同一图像的高度，𝑊为同一图像的宽度，‖𝑙1-norm‖分别为1,max(⋅)为分步最大选择。利用最大选择策略对红外图像和可见光图像的像素强度分布进行整合。然后，利用积分分布约束融合图像的像素强度分布。 

我们期望融合后的图像能保持最优的强度分布，同时保留源图像丰富的纹理细节。而强度损失仅为模型学习提供了粗粒度分布约束。因此，在融合图像中引入纹理损失，使融合图像包含更多的细粒度纹理信息。纹理损失定义为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673849865282.png" alt="1673849865282" style="zoom:80%;" />

 其中∇表示Sobel梯度算子，用于测量图像的细粒度纹理信息。|⋅|为绝对手术。我们假设融合后的图像的最优纹理是红外图像和可见光图像纹理的最大集合。 

综上所述，我们的基于梯度残差密集块的融合网络在内容丢失的指导下，既能获得最优的强度分布，又能保留丰富的细节信息。也就是说，内容损失可以有效地保证我们的模型达到第一个目标，即提高融合图像的视觉质量和统计评价指标。

######3.2.2 语义损失

充分增强融合图像的语义信息是我们算法最大的创新。为了实现这一目标，我们创造性地设计了一个语义缺失。具体来说，我们引入了实时语义分割模型[52]对融合后的图像进行分割，分割网络输出分割结果𝐼𝑠∈R𝐻×𝑊×𝐶，辅助分割结果𝐼𝑠𝑎∈R𝐻×𝑊×𝐶。语义损失由两个要素组成，即主语义损失和辅助语义损失。主要语义损失和辅助语义损失定义如下:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673850094435.png" alt="1673850094435" style="zoom:80%;" />

其中𝐿𝑠𝑜∈R𝐻×𝑊×𝐶表示分割标签𝐿𝑠∈(1，𝐶)𝐻×𝑊变换后的一个热向量。主语义丢失和辅助语义丢失从不同角度反映了融合图像中所包含的语义信息。最终，语义损失表现为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673850121196.png" alt="1673850121196" style="zoom:80%;" />

其中𝜆是平衡主语义损失和辅助语义损失的常数，根据原论文[52]设置为0.1。值得注意的是，在对融合网络进行约束的同时，语义丢失也被用来训练分割模型。更详细的分割模型的损失函数和网络架构的描述参考[52]。

最后，构造一个关节损耗来指导融合模型的训练，定义为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673850158229.png" alt="1673850158229" style="zoom:80%;" />

其中𝛽是描述语义丢失重要性的超参数![1673850188458](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673850188458.png)。需要强调的是，随着训练的进行，分割网络会随着融合模型的发展而变得自适应，𝛽会根据低层和高层的联合自适应训练策略逐步增加。

![1673850224538](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673850224538.png)

<center>图3 基于梯度残差稠密块的实时红外与可见光图像融合网络体系结构。</center>
<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673850579181.png" alt="1673850579181" style="zoom:80%;" />

<center>图4 梯度残差密集块体的具体设计。选择Sobel算子作为梯度算子，提取特征图的细粒度细节信息。</center>
##### 3.3 网络架构

为了实现实时的图像融合，我们提出了一种基于GRDB的轻量级红外与可见光图像融合网络，如图3所示。我们的融合网络由一个特征提取器和图像重构器组成，其中特征提取器包含两个grdb来提取细粒度的特征。

如图3所示，该特征提取器包含两个平行的红外和可见光特征提取流，每个特征提取流包含一个公共卷积层和两个grdb。采用核大小为3 × 3、激活函数为LReLU (Leaky Linear Unit)的公共卷积层提取浅层特征。接下来是两个用于从浅层特征中提取细粒度特征的grdb。GRDB的具体设计如图4所示。梯度剩余密集块是resblock[26]的变体，其中主流采用密集连接，剩余流采用梯度操作。由图4可以看出，主流采用LReLU部署了2个3 × 3卷积层和1个普通卷积层，其内核大小为1 × 1。需要强调的是，我们将密集连接引入主流，以充分利用卷积层提取的特征。剩余流采用梯度运算来计算特征的梯度大小，采用1 × 1正则卷积层来消除通道维差。然后，通过逐元素添加的方式添加主密集流和剩余梯度流的输出，整合深度特征和细粒度细节特征。

随后，通过拼接策略将红外图像和可见光图像的细粒度特征进行整合，并将结果反馈到图像重构器中，实现特征聚合和图像重构。图像重构由三个串联的3 × 3卷积层和一个1 × 1卷积层组成。3 × 3卷积层均采用LReLU作为激活函数，而1 × 1卷积层的激活函数为Tanh。

众所周知，在图像融合任务中，信息丢失是一个灾难性的问题。因此，我们的融合网络中的填充设置相同，除了1 × 1的卷积层外，stride设置为1。因此，我们的网络没有引入任何下采样，融合后的图像大小与源图像一致。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673850738550.png" alt="1673850738550" style="zoom:80%;" />

##### 3.4 低水平和高水平的联合适应性训练策略

现有的任务驱动的低水平视觉方法要么采用预先训练好的高水平模型来指导低水平视觉任务模型的训练，要么将低水平和高水平视觉任务模型在一个阶段联合训练。然而，在图像融合领域，很难提供融合后图像的地面真实度来训练高级视觉任务模型。此外，单阶段联合训练策略可能会导致在低水平和高水平视觉任务之间难以保持表现平衡。为此，我们设计了一个低水平和高水平联合训练策略来训练我们的融合网络。具体来说，我们对融合网络和分割网络进行迭代训练，迭代次数设置为𝑀。首先，在关节损耗的引导下，利用Adam优化器对融合网络中的所有参数进行优化。通过迭代动态调整关节损耗超参数𝛽，其表达式为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673850793340.png" alt="1673850793340" style="zoom:80%;" />

其中𝑚表示𝑚th迭代。𝛽随着训练的进行逐渐增加，这是因为随着迭代次数的增加，分割网络对融合模型的拟合更好，语义丢失可以更准确地指导融合网络的训练。𝛾是平衡语义损失和内容损失的常量。然后，根据当前融合结果，通过优化语义损失来更新分割模型的参数。在每次迭代中，融合模型和分割模型的训练步骤分别为𝑝和𝑞。算法1总结了低水平和高水平自适应联合训练策略。

####四. 实验验证

在本节中，我们首先提供实验配置和实现细节。然后，我们给出了一些比较实验和推广实验来揭示我们所提出的航海的优越性。此外，还进行了任务驱动的评估实验，从高级视觉任务的角度对不同的融合方法进行评估。接下来，我们比较了不同方法的运行效率，以验证我们的轻量级网络在实时图像融合方面的优越性。最后，通过对语义丢失、梯度残差密集块和关节低、高水平自适应训练策略的研究，验证了该方法的有效性。

![1673850849525](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673850849525.png)

<center>图5  对来自MFNet数据集的00537D图像进行了SeAFusion与9种最先进的方法的定性比较。</center>
##### 4.1 实验配置

为了综合评价所提出的算法，我们在MFNet[5]、RoadScene[42]和TNO[53]数据集上进行了大量的定性和定量实验。我们将该方法与9种最新的方法进行了比较，包括两种传统方法GTF[20]和MST-SR[37]，两种基于ae的方法DenseFuse[22]和RFN-Nest[6]，两种基于gan的方法fusongan[27]和GANMcC[47]，以及三种基于cnn的方法IFCNN[24]、U2Fusion[42]和SDNet[47]。所有这9个方法的实现都是公开可用的，我们按照原始论文中报道的那样设置参数。值得注意的是，我们采用拉普拉斯金字塔(Laplace pyramid, LP)作为多尺度变换(multi-scale transformation, MST)。对DenseFuse、IFCNN和RFN- nest分别采用逐元素添加、逐元素最大融合策略和残差融合网络(RFN)来整合深度特征。

选取熵(EN)[54]、互信息(MI)[55]、视觉信息保真度(VIF)[56]、空间频率(SF)[57]、标准差(SD)和𝑄𝑎𝑏𝑓等6个统计评价指标进行量化评价。EN测量融合图像中包含的信息量，MI计算从源图像传输到融合图像的信息量。enn和MI都是从信息论的角度评价融合性能。VIF从人眼视觉系统的角度评价融合后图像的信息保真度。SF测量融合图像中包含的空间频率信息。SD从统计学的角度反映融合图像的分布和对比度。𝑄𝑎𝑏𝑓计算从源图像传输到融合图像的边缘信息量。EN、SF和SD是无参考指标。此外，具有较大EN、MI、VIF、SF、SD和𝑄𝑎𝑏𝑓的融合算法具有较好的融合性能。

#####4.2  实现细节

我们在MFNet数据集上训练我们的语义感知融合网络。训练集包含1083对红外和可见光图像，测试集包含361对图像。MFNet数据集为9个对象提供了语义标签，即汽车、人、自行车、曲线、停车、护栏、色调和背景。此外，所有图像在输入网络之前都被归一化为[0,1]。

根据高、低层次联合自适应训练策略，对融合网络和分割网络进行迭代训练。我们的联合自适应训练策略中所有参数设置如下:𝑀= 4，𝑝= 2,700，𝑞= 20,000，𝛾= 1。另外，内容丢失的超参数设置为𝛼= 10。我们利用批量大小为8的Adam优化器，𝛽1为0.9，𝛽2为0.99,epsilon为1𝑒−8，权值衰减为0.0002，初始学习率为0.001，以关节丢失为指导来优化我们的融合模型。此外，我们利用小批量随机梯度下降法，批量大小为16，动量为0.9，权值衰减为0.0005来优化分割网络。初始学习率设置为0.01，学习率通过初始学习率乘以(1−𝑖𝑡𝑒𝑟𝑚𝑎𝑥𝑖𝑡𝑒𝑟)𝑝𝑜𝑤𝑒𝑟来更新，其中power设置为0.9[52]。该方法在PyTorch平台[58]上实现。所有实验都是在NVIDIA TITAN RTX GPU和3.50 GHz Intel(R) Core(TM) i9-9920X CPU上进行的。

由于MFNet和RoadScene数据集包含了彩色可见图像，因此我们使用了一种特殊的策略[59]来处理颜色信息。更具体地说，我们首先将可见图像转换为YCbCr颜色空间。然后利用不同的融合算法对可见光图像和红外图像的Y通道进行融合。最后，将融合后的图像转换成具有Cb和Cr通道的RGB颜色空间。此外，我们将融合后的RGB图像直接输入分割模型中。

##### 4.3 对比实验

为了充分评估我们的方法的融合性能，我们首先在MFNet数据集上比较提出的海员融合算法和其他9种算法。

######4.3.1 定性结果

MFNet数据集包含两个典型场景，即白天场景和夜间场景。为了直观地展示我们的融合框架在整合互补信息和提高融合图像视觉质量方面的优势，我们选择了两个白天场景和两个夜间场景进行主观评价。可视化结果如图5-8所示。在白天场景中，可以利用红外图像的热辐射信息作为可见光图像的补充信息。因此，融合后的图像应包含丰富的可见图像纹理细节，增强红外图像中的突出目标。如图5和图6所示，GTF和fusongan不能保持可见光图像的纹理细节，而fusongan不能对突出目标的边缘进行锐化。虽然DenseFuse、RFN-Nest、GANMcC、U2Fusion和SDNet将可见光图像的细节信息与红外图像的热辐射信息进行了融合，但这两类信息在融合过程中不可避免地会受到无用信息的干扰。我们放大一个带有红色框的区域，以说明纹理细节受到不同程度光谱污染的现象。此外，一个突出的区域用绿色框突出显示，显示无用信息削弱突出目标的问题。只有我们的SeAFusion和MST-SR能够在突出突出目标的同时保留丰富的纹理细节。然而，MST-SR在某些背景区域(如图5、图6中的地面)容易受到热辐射信息的污染，因此，只有我们的方法才能有效地整合源图像中的互补信息，同时保证融合后图像的视觉质量。

在夜间场景中，无论是红外图像还是可见光图像都只能提供有限的场景信息。因此，如何从红外图像和可见光图像中自适应地整合有意义的信息是一个挑战。如图7和图8所示，我们可以看到，所有算法都在一定程度上融合了红外和可见光图像中的互补信息，但不同算法的融合结果仍有细微的差异。特别是GTF和fusongan模糊了热辐射目标的轮廓，GTF的纹理区域受到严重的光谱污染。除本方法外，其他方法在融合后的图像中引入了一些无用的信息，表现为纹理细节的污染和显著目标的弱化。我们放大一个有纹理的区域(即红框)并突出显示一个目标(即绿框)以显示上述问题。值得强调的是，我们的SeAFusion在语义丢失的引导下，有目的地整合源图像中的有意义信息，生成包含丰富语义信息的融合图像。此外，该融合模型合成的融合图像包含丰富的纹理细节，这得益于梯度残差稠密块对细粒度细节的强大描述能力。

###### 4.3.2 定量结果

6个统计指标对361幅图像对的定量结果如图9所示。可以注意到，我们的方法在EN、MI、VIF和𝑄𝑎𝑏𝑓四个指标上显示出显著的优越性。其中，EN值最高表示融合后的图像包含的信息最多，MI值最高表示融合后的图像中包含的信息最多。此外，融合后的图像具有较好的VIF，说明融合后的图像更符合人类视觉系统。此外，本文方法获得了最好的𝑄𝑎𝑏𝑓，这意味着融合结果中保留了更多的边缘信息，这得益于GRDB强大的细粒度特征提取能力。此外，我们的SeAFusion显示出最好的SD，这意味着我们融合的图像具有最高的对比度。我们的方法在SF度量中只遵循IFCNN和MST-SR的一小部分。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673856376981.png" alt="1673856376981" style="zoom:80%;" />

<center>图8。对来自MFNet数据集的01024N图像进行了SeAFusion与9种最先进的方法的定性比较</center>

##### 4.4 泛化实验

众所周知，泛化性能是评估基于深度学习的方法的一个重要方面。因此，我们在道路场景和TNO数据集上提供了泛化实验，以证明所提出的海员方法的泛化性。值得注意的是，我们的融合模型是在MFNet数据集上训练的，并直接在道路场景和TNO数据集上测试。

######4.4.1 定性结果

不同算法在道路场景数据集上的定性比较如图10和图11所示。几乎所有的方法在融合过程中都引入了无意义的信息，表现为纹理区域受到热辐射污染和显著目标减弱。为了直观地展示融合图像中无意义信息的效果，我们在红框中放大纹理细节丰富的区域，在绿框中突出突出的目标。我们可以观察到背景区域的纹理细节受到热辐射信息的干扰。其中GTF、DenseFuse、RFN-Nest、fusongan、GANMcC和SDNet尤为明显。同时，显著目标的强度信息被不同程度地削弱。GTF和fusongan不能保留目标的锋利边缘。值得一提的是，MST-SR、IFCNN、U2Fusion和SeAFusion只受到少量无用信息的干扰。特别地，融合结果在背景区域与可见光图像相似，显著目标的像素强度与红外图像一致。

不同方法对TNO数据集的可视化结果如图12和图13所示。从绿框中可以看出，MST-SR、DenseFuse、RFN-Nest和U2Fusion严重削弱了显著目标。此外，fusongan和GANMcC模糊热目标的边缘。此外，其他方法生成的融合图像在背景区域存在严重的光谱污染，如图13中的灌木。只有我们的方法能够成功地保持可见光图像的纹理细节和显著目标的强度。

###### 4.4.2 定量结果

我们还分别从RoadScene和TNO数据集中选取了25对图像进行定量评价。不同方法对6个指标的比较结果如图14、图15所示。从图14中可以看出，在RoadScene数据集上，SeAFusion在EN、MI、VIF、SF和SD方面表现出了明显的优势。这意味着融合后的图像不仅包含了丰富的信息和纹理细节，而且具有最高的对比度和最佳的视觉质量。此外，SeAFusion在𝑄𝑎𝑏𝑓中排名第二，说明我们的方法从源图像到融合图像传递了足够的边缘信息。

如图15所示，在TNO数据集上，seausion在EN、MI、VIF和𝑄𝑎𝑏𝑓指标上均排名第一，优势显著。对于SD指标，我们的方法仍然排名第一，尽管优势不是特别明显。最后，在SF度规上，该方法仅与IFCNN有较窄的差距。

总之，在各种数据集上的大量定性和定量结果证明了我们的方法在显著目标保持和纹理保持方面的优越性。我们将其优势归结于以下几个方面。一方面，我们定义了包含强度损失和纹理损失的内容损失，从像素强度分布和高阶纹理细节的角度约束融合网络，有效整合有意义的信息;另一方面，我们的融合网络可以在语义丢失的引导下，将互补特征与语义信息自适应融合。最后，利用精细梯度残差稠密块增强网络对细粒度细节的描述能力。

#####4.5 任务驱动评估

融合后的图像不仅可用于视觉观察，还可用于高级视觉任务。然而，现有的评价方法只关注融合图像的视觉质量和统计指标。在本节中，我们打破了现有评估方式的限制，提出了任务驱动的评估标准。具体来说，我们对融合后的图像进行语义分割和目标检测，计算不同融合方法的分割或检测性能。

######4.5.1 分段表现

为了公平比较，我们在原始MFNet数据集上针对不同的融合算法重新训练分割网络[52]。培训和测试集的配置与用于培训我们的SeAFusion的配置一致。首先，利用每种融合方法生成融合图像。然后，分别在红外、可见光和融合图像训练集上对11个分割模型进行80000步的训练。我们的SeAFusion采用了联合的低层次和高层次自适应训练策略，对融合模型和分割模型进行联合训练，分割网络也进行了80,000步的训练。通过像素相交并集(IoU)来衡量分割性能。细分驱动的评价结果如表1所示。可以看出，我们的算法在几乎所有类别中都获得了最高的IoU，在mIoU中排名第一。我们把优势归结为两点。该融合网络一方面完全融合了红外图像和可见光图像的互补信息;互补信息有助于分割模型对成像场景的全面理解，这是融合能够提高分割性能的重要原因。另一方面，在语义丢失的指导下，自适应地整合有意义/语义信息。因此，融合后的图像包含丰富的语义信息，使得分割网络能够更准确地描述图像场景。我们认为增强融合图像的语义信息是我们的方法在分割性能上优于其他算法的核心因素。

此外，我们还提供了一些可视化的例子来展示在红外、可见光和不同融合图像上的分割结果。我们只给出了GTF、fusongan、DenseFuse、IFCNN和SDNet这五种具有代表性的融合算法的分割结果，如图16所示。从结果中我们可以发现，红外图像可以提供更多的关于行人等突出目标的信息，而可见光图像可以提供更好的背景描述(如汽车、自行车和色锥)。优秀的融合算法能够整合源图像中的互补信息，实现对成像场景的更全面描述。因此，该分割模型可以在融合后的图像上获得较好的分割效果。值得一提的是，我们的融合方法在融合过程中充分融合了源图像的语义信息。因此，分割模型可以对融合后的图像产生更精确的分割结果，例如00127D场景下的汽车，00504D场景下的颜色锥和自行车，01066N场景下的自行车。

![1673857101674](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673857101674.png)

<center>图9。对MFNet数据集上的361对图像进行6个度量指标(EN、MI、VIF、SF、SD和𝑄𝑎𝑏𝑓)的定量比较。曲线上的一个点(𝑥，𝑦)表示有100%的∗𝑥%的图像对其公制值不超过𝑦。</center>

<center>表1：基于MFNet数据集的可见光、红外和融合图像分割性能(mIoU)。红色表示最佳结果，蓝色表示次之结果。</center>

![1673857151010](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673857151010.png)

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673857223002.png" alt="1673857223002" style="zoom:80%;" />

<center>图10。在RoadScene数据集的FLIR_06832上，将seausion与9种最先进的方法进行定性比较。</center>

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673859534031.png" alt="1673859534031" style="zoom:80%;" />

<center>图11。在RoadScene数据集的FLIR_08835上，将seausion与9种最先进的方法进行定性比较。</center>

<center>表2：基于MFNet数据集的可见光、红外和融合图像的目标检测性能(mAP)。最好的结果用红色表示，次好的结果用蓝色表示。</center>

![1673859586665](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673859586665.png)

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673859652653.png" alt="1673859652653" style="zoom:80%;" />

<center> 图12 :在TNO数据集的Kaptein_1123上，将seausion的可视化结果与9种最先进的算法进行比较。 </center>

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673859706478.png" alt="1673859706478" style="zoom:80%;" />

<center>图13:在TNO数据集Tree_4915上，将seausion的可视化结果与9种最先进的算法进行比较。</center>

#####4.5.2 检测性能

目标检测作为一种通用的高级计算机视觉任务，其性能也很好地反映了融合图像中所集成的语义信息。因此，我们采用了一种最先进的检测器YOLOv5[62]来评估融合图像的目标检测性能。我们从MFNet数据集中随机选取80幅图像作为测试集，这些图像几乎描述了所有的城市场景。我们用两个关键的类别来手工标注这些图像，即人和车。将红外图像、可见光图像和各种融合方法的融合结果分别直接输入YOLOv5检测器。利用平均平均精度(mAP)来衡量检测性能。检测驱动的评价结果如表2所示，其中mAP@0.5、mAP@0.7、mAP@0.9分别为IoU阈值0.5、0.7、0.9处的mAP值，mAP@[0.5:0.95]为不同IoU阈值处mAP值的平均值(0.5 ~ 0.95，步长为0.05)。

结果表明，在几乎所有IoU阈值下，红外图像对人的mAP值都是最好的，这意味着红外图像能够为检测器提供足够的关于显著目标(如人)的语义信息。然而，红外图像在汽车上的检测结果令人失望。幸运的是，可见图像可以为检测器提供大量关于汽车的语义信息。不同的融合算法可以融合红外图像和可见光图像的互补信息，使得融合后的图像在汽车上的检测性能是令人满意的。然而，融合过程中的不相关信息会削弱显著目标，因此融合后的图像与红外图像相比，对人的检测结果是降级的。值得强调的是，我们的语义感知融合框架能够在内容丢失的指导下实现强度维护和纹理保存，在语义丢失的指导下实现对无意义信息干扰的防御。因此，我们对人体的检测性能仅落后于红外图像一个很小的差距。我们的融合结果在汽车检测和平均检测精度方面领先于其他同类产品。

此外，我们还在图18中提供了一些可视化的例子来说明我们的融合算法在便于目标检测方面的优势。在00479D场景中，由于光照因素，检测器无法从可见光图像中检测出行人。而GTF和fusongan不能保持行人的锐化边缘，DenseFuse和RFN-Nest由于负面信息的干扰而削弱显著目标。因此，检测器也无法从上述方法生成的融合图像中检测出人。相反，我们的方法可以充分整合源图像的语义信息，同时保持最优的强度分布和丰富的纹理细节。因此，该检测器从融合后的图像中检测出所有目标，并提高了与源图像相比的置信度。在00689N场景中也出现了类似的现象。值得注意的是，我们的融合结果显著提高了检测结果的置信度，这表明我们的融合图像可以为检测器提供更多的语义信息。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673859826596.png" alt="1673859826596" style="zoom:80%;" />

<center>图14。对RoadScene数据集中的25对图像进行6个指标(即EN、MI、VIF、SF、SD和𝑄𝑎𝑏𝑓)的定量比较。曲线上的一个点(𝑥，𝑦)表示有100%的∗𝑥%的图像对其公制值不超过𝑦。</center>

![1673866420844](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673866420844.png)

<center>图15  对来自TNO数据集的25对图像进行6个度量指标(EN、MI、VIF、SF、SD和𝑄𝑎𝑏𝑓)的定量比较。曲线上的一个点(𝑥，𝑦)表示有100%的∗𝑥%的图像对其公制值不超过𝑦。</center>

表3各方法在MFNet、RoadScene和TNO数据集上运行时间的均值和标准差(单位:秒，RED表示最佳结果，BLUE表示次优结果)。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673866484988.png" alt="1673866484988" style="zoom:80%;" />

#####4.6 效率比较

如上所述，我们的融合模型是一个轻量化的网络，可以实现实时的图像灌注。为此，我们在表3中提供了不同算法的平均运行时间来证明我们的效率优势。我们可以发现，所有基于深度学习的算法都具有显著的运行效率，这得益于GPU的加速。此外，我们的SeAFusion是对这三种数据集最快的方法。我们将这种优势归因于两个因素:轻量级的网络设计和基于pytorch的算法实现。因此，我们的算法可以很容易地被部署为高级视觉任务的预处理模块。

#####4.7 消融研究

######4.7.1 语义损失分析

我们提出的轻量化融合网络可以在语义丢失的指导下增强融合图像中的语义信息。为了验证语义丢失的特殊作用，我们设计了语义丢失的消融研究。更具体地说，我们只在内容丢失的指导下训练融合模型。一些典型的例子如图19所示。我们可以注意到，如果没有语义丢失的引导，融合网络不能有目的地保留源图像的有意义信息。这具体表现在背景区域纹理细节的平滑和显著目标的弱化。我们还在表4中提供了分割结果，其中Without Semantic Loss表示我们只利用内容丢失来训练融合网络，以证明语义丢失对于促进高级视觉任务的重要作用。我们只提供个人、汽车、自行车的欠条，以及所有类别的欠条。可以发现，在没有语义丢失的指导下，融合图像的分割性能显著下降。相比之下，我们的融合算法在有效提高融合图像分割性能的同时，实现了显著的目标强度保持和纹理保持。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673866615668.png" alt="1673866615668" style="zoom:80%;" />

<center>图16。基于MFNet数据集的红外、可见光和融合图像的分割结果。在红外图像集、可见光图像集和融合图像集上对分割模型进行再训练。每两行代表一个场景，从上到下分别为:00127D, 00504D和01066N。</center>

######4.7.2 梯度残差密集块体分析

我们融合网络中的另一个关键组件是GRDB，它增强了网络对细粒度细节的描述能力。为此，我们还对GRDB进行了消融研究，并将可视化结果显示在图19中。在消融实验中，我们从GRDB中去除残余梯度流。从可视化的例子中，我们可以发现融合后的图像能够保持适当的强度分布，但不能有效地保持背景中的纹理细节。相反，该方法在保持显著目标强度分布的同时，增强了纹理细节的描述。

综上所述，在提高融合图像分割性能的同时，我们有意保持了图像的强度分布和纹理细节，这得益于我们特殊的设计，即语义丢失和梯度残差密集块。

<center>表4消融研究和不同训练策略的分割性能。红色表示最佳结果，蓝色表示次之结果。</center>

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673866721485.png" alt="1673866721485" style="zoom:80%;" />

#####4.8 培训方法比较

我们提出了一种低水平和高水平的联合自适应训练策略来全面提高融合和分割的性能。在本节中，我们设计了训练策略对比实验，以证明所提出的低水平和高水平联合自适应训练策略的有效性。具体而言，在提出的训练策略的基础上，利用单阶段联合训练策略对融合网络进行训练。然后分别在红外图像集和可见光图像集上训练分割模型，指导融合网络的训练。因此，使用不同训练策略训练的三种融合模型被用作替代方案。融合结果和分割结果如图19和表4所示，其中，Without Semantic Loss表示我们只利用内容丢失来训练融合网络，pre- training on Infrared和pre- training on Visible表示我们分别利用在红外和可见光图像数据集上训练的分割模型来指导融合模型的训练，单阶段联合训练是指采用单阶段联合训练策略对融合网络和分割模型进行联合训练。可以注意到，采用单阶段联合训练策略训练的模型提高了分割性能，但降低了融合图像的视觉质量。此外，分割模型引导融合模型，对可见图像集进行训练，忽略了突出目标。同样，使用红外图像训练的分割模型来指导融合网络的训练也会导致纹理细节信息的丢失。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673866783159.png" alt="1673866783159" style="zoom:80%;" />

<center>图18。基于MFNet数据集的红外、可见光和融合图像的目标检测结果。在Coco数据集上部署预先训练的YOLOv5检测器，实现目标检测。每两行代表一个场景，从上到下分别为:00479D和00689N。</center>

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1673866821526.png" alt="1673866821526" style="zoom:80%;" />

<center>图19 消融研究和不同训练策略的可视化结果。</center>

####五.结论

在本研究中，提出了一种语义感知的图像融合框架，称为SeAFusion，以实现实时红外和可见光图像的融合。一方面，设计了梯度残差稠密块，提高了融合网络对细粒度细节的描述能力;结合精细内容的丢失，我们的融合网络有效地实现了显著目标强度的保持和纹理细节的保存。另一方面，我们引入语义丢失来提高高级视觉任务融合结果的易化程度。更具体地说，语义丢失使得高层次的语义信息回流到图像融合模块，有利于高级视觉任务在融合结果上获得更好的性能。此外，我们提出了一种低水平和高水平的联合自适应训练策略，以在图像融合和各种高级视觉任务中同时获得令人印象深刻的表现。广泛的比较和泛化实验表明，我们的海员在主观效果和定量指标方面都优于最先进的竞争对手。此外，大量的任务驱动评估实验揭示了我们的框架在促进高层次愿景任务方面的天然优势。此外，我们的算法在运行效率上的显著优势，使其可以很容易地部署为高级视觉任务的预处理模块。