### Transformer-Based端到端解剖和功能图像融合

**摘要：**医学图像融合旨在从不同模态的医学图像中获取互补信息，在临床应用中变得越来越重要。融合策略的设计对于实现高质量的融合结果起着关键作用。==现有方法通常采用手工融合规则或基于卷积的网络来融合多模态医学图像。然而，这些融合策略不够细粒度，无法有效捕获多模态图像的全局信息。==此外，对于基于深度学习的融合方法，它们总是将源图像/不同模态的深层特征连接起来输入到神经网络中，容易导致源图像的信息利用不足。为了解决这些问题，我们提出了一种用于医学图像融合的基于 transformer 的端到端框架，称为 TransFusion。所提出的 TransFusion 引入了 transformer 作为融合策略，它利用其自我注意机制来合并多模态特征的全局上下文信息并充分融合它们。此外，与传统的并行多分支架构或用于多个输入的共享网络不同，我们设计的分支网络通过多尺度的融合转换器进行交互，以更充分地利用不同模态的信息。我们设计的一个天然优势是能够通过自注意力聚合全局多模态特征。定性和定量实验都证明了我们的方法优于最先进的融合方法。

#### 介绍

医学成像技术可以传达独特的数字和补充对身体的理解和组织结构。**根据图像信息,多模态医学图像可以分为解剖和功能图像[1]。**==解剖图像具有较高的空间分辨率,可以清晰地图像器官的解剖结构,但不能显示人体新陈代谢的功能变化。磁共振(MR)成像是典型的解剖形态,它提供了软组织的信息。==相反,功能图像可以反映功能和代谢信息,但无法描述由于低分辨率解剖细节,如正电子发射断层扫描(PET)和单光子发射断层扫描(SPECT)。由于single-modal医学图像的局限性,多模态医学图像融合吸引了越来越多的注意力。它的目标是医学图像不同的方式融合成一个单一的图像,包含从源模式互补信息,如图1所示。医学图像融合可以为临床诊断、手术导航,和治疗计划[2]。

实现图像融合通常包括三个过程:特征提取、融合,重建。**在传统的方法中,各种分解/转换设计方法从图像中提取特征不同的模式。然后,提取的特征融合的融合策略。最后,一个执行逆变换重建到融合图像的融合特性。**在这个方向,提出了大量的传统方法[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14],[15],[16]。基于典型的包括稀疏表示(SR)的方法[3]、[4]、[5],多尺度decomposition-based方法,如金字塔[6]、[7],[8]和小波[9],[10],[11],和突出特点方法[13],[14],[15]。

虽然传统的融合方法取得了令人满意的融合效果,但其局限性挺大。第一,融合性能高度依赖预定义的功能,限制其他融合方法的推广任务。第二,不同的功能可能需要不同的融合策略。第三,SR-based方法,字典学习非常耗时,融合一副图像的时间成本太高。

最近,基于深度学习融合方法。他们可以避免上述传统方法的缺点。获得高质量的图像从图像融合不同的模式,需要考虑两个方面。==首先是利用多通道图像的信息互补。基于当前深入学习融合方法通常连接源图像/深度不同的形式输入到神经网络的特点和维护所需的信息,一些约束[17],[18],[19],[20],[21],[22]。然而,简单的连接可能不会有效地利用多通道图像信息,从而限制了融合性能。==**第二个是合适的融合策略的设计。现有融合方法途径采用结构或手工制作的融合规则融合特性不同的模式。这些融合策略难以捕获全局上下文信息的多通道图像特性,这很容易导致图像质量下降。**值得注意的是,由于低分辨率功能图像马赛克现象和不清楚的边缘,同时解剖图像有较高的分辨率。充分融合图像有不同的决议,应考虑以下:1)能辨别不同的局部区域的语义信息,2)的边缘信息可以显示不同区域之间的边界。这两种都是全局的信息,他们有一个关系在语义信息[23]。尽管卷积神经网络(CNN)的融合方法通过多个将采样和卷积过程可以获得全局对图像的理解,但是他们无法有效地捕获全局语义信息。因此,一个适当的融合策略的设计仍然是一个问题。

基于上述分析,本文提出了一种新的融合方法,称为TransFusion。这项工作的主要贡献是总结如下。

* 我们提出一个端到端的非监督的解剖和功能图像融合的方法,介绍了一种Transformer的融合策略。提出融合Transformer利用self-attention机制将多模态医学图像的全局上下文信息和充分融合他们的语义信息,实现细粒度的融合效果。
* 我们设计交互的分支网络通过在多尺度融合Transformer。通过这种方式,对源图像进行多尺度互补特性提取的特征不同的模式可以充分利用比使用共享网络和并行multi-branch架构。
* 我们评估该方法在多个多模态医学图像数据集和展示更好的融合性能较先进的融合方法。

##### 相关工作

本节介绍了当前基于深度学习医学图像融合方法。目前,医学图像融合领域的流行的深层神经网络是cnn和生成对抗网络(GAN)。接下来,这一节还介绍了变压器。

###### 基于深度学习医学图像融合方法

基于 CNN 的方法：一些融合方法将 CNN 与手工分解方式的融合策略相结合，以实现医学图像融合 [24]、[25]。在 [24] 和 [25] 中，Siamese CNN 用于生成源图像的权重图。然后，权重图和源图像通过金字塔分解方式进行分解。最后，设计了手工融合规则和金字塔重建策略以获得融合图像。由于成像设备的限制，参考融合图像自然是不存在的。没有基本事实来监督模型进行训练。因此，图像融合本质上是一项无监督的任务。许多端到端的基于 CNN 的医学图像融合方法已经被开发出来，通过最小化源图像和融合输出之间的损失函数直接输出融合图像，而无需手工设计 [17]、[18]、[19] , [26]。由于医学图像融合的ground truth不可用，这些方法主要关注源图像特征提取的设计，以约束保留的信息。其中，徐等人。 [17]、[18] 以及 Xu 和 Ma [19] 沿通道维度连接源图像并将它们输入 DenseNet 以获得融合图像。为了限制从源图像中提取的信息，Xu 等人。 [17] 采用图像质量评估网络和熵来测量源图像的图像质量和丰度，Xu 等人。 [18] 通过预训练的 VGG 网络测量从源图像中提取的特征的梯度信息量，EMFusion [19] 进一步提出了表层和深层约束。在表面层面，计算图像像素阈值和熵来衡量源图像的显着性和丰度。在深层约束下，使用重建网络提取源图像的独特信息。虽然 EMFusion 通过设计客观和主观活动水平约束实现了具有竞争力的融合性能，但其对源图像的简单连接是粗粒度的，在一定程度上限制了融合性能。 Tang 等人不是简单的连接，而是。 [26] 通过交叉方式开发了两个交互的 CNN，以融合结构和功能医学图像。然而，基于 CNN 的融合策略在捕获多模态医学图像的全局上下文时很脆弱。

基于GAN的方法: GAN 包含两个主要组件: 生成器和鉴别器，分别负责生成样本并区分输入是真实的还是假的。 GAN 在生成器和鉴别器之间建立对抗性游戏，以改进彼此。 针对医学图像融合 [ 20]， [21]， [22] 提出了一些基于 GAN 的方法。 DDcGAN [20] 为多分辨率图像融合设计一个生成器和两个鉴别器。 将具有不同分辨率的两个源图像在解卷积后级联，并输入到基于 DenseNet 的生成器，以输出融合图像。 一个鉴别器负责区分融合图像和高分辨率源图像，另一个鉴别器用于区分下采样融合图像和低分辨率源图像。 它可以约束融合图像以包含来自两个源图像的信息。 DSAGAN [22] 引入了除双流生成器和两个鉴别器之外的注意机制以提高融合质量。 同样，这些基于 GAN 的融合方法也与 CNN保持着相同的缺点。

##### B. Transformer 

Transformer 最先被提出用于机器翻译 [27]，并在自然语言处理 (NLP) 任务中取得了巨大成功。 Transformer 的关键是 multi-head self-attention，它聚合了每个标记的特征。与 CNN 相比，transformer 的一个天然优势是能够捕获全局上下文。受NLP领域的启发，图像分类[28]、[29]、图像分割[30]、[31]和目标检测[32]、[33]等计算机视觉领域引入了transformer结构并获得先进的性能。目前，基于transformer的模型在医学图像融合领域并不多见。已知的方法包括 IFT [34]。 IFT曾尝试将transformer结构引入医学图像融合中。它提出了一个新的融合模块，由并行的 CNN 和 transformer 分支组成，分别负责从源图像中提取局部和远程特征。然而，由于固定编码器，IFT 中多模态图像的信息利用不足。 

##### 3方法

在本文中,我们实现两个医学图像融合任务:1)MR-T1和PET)MR-T2 和 SPECT。让MR(MR-T1 / MR-T2)大小的图像表示S~1~，大小为W * H * C1和PET / SPECT图像表示 S~2~大小W* H × C2。H和W分别是高度和宽度。通道的数量C1 = 1和C2 = 3。我们制定这些融合任务学习之间的映射(S1和S2)和S ~f~通过深度学习模型。S ~f~是融合的图像大小W *H *C~f~, C ~f~ = max (C1, C2)。

PET / SPECT图像的功能(颜色)信息不可用在MR图像(灰度图像)。因此,我们将PET / SPECT图像从RGB空间到YCbCr空间,这是计算(1)。然后,Y(亮度)通道是通过该模型融合MR图像。最后,输出图像结合色度通道Cb和PET / Cr SPECT图像得到最终的融合图像。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672628485553.png" alt="1672628485553" style="zoom:80%;" />

提拟议的融合网络TransFusion是端到端和无监督的，其框架见图2。 TransFusion由三个组件组成：编码器、融合变压器和解码器。 重要符号缩写的含义见“命名”。 值得注意的是，端到端过程涉及色域转换操作，这是医学图像融合领域的一种正常数据处理。

编码器由两个互动的分支网络组成，每个网络遵循RFN-Nest [35]中的编码器架构。 它用于从不同模式的图像中提取综合特征。 “m×n，c”表示内核大小为m×n和内核数c的卷积层。 所有最大池化层都使用内核大小为2×2且步长为 2。 因此，可以提取源图像中的多尺度特征。 浅卷积层提取的特征将保存更详细的信息，深层提取的特征包含高层语义信息，这对于重建突出特征至关重要。 我们介绍Transformer在每个尺度上结合多模式深度特征的全局背景。 然后，Transformer的每个输出在元素总结与原始模式特征后被反馈到相应的模式分支中。 每项输出都将其他模式的信息纳入其相应模式的特征，为模式分支中的下列特征提取提供了更多的补充信息。 所有产出的基本总结被认为是融合特征，它包含多模式全球环境，对于重建更具信息性的融合图像非常重要。 最后，采用解码器来重建从所有尺度的融合特征融合图像。 解码器网络架构遵循RFN-Nest [35]，它可以充分利用基于巢连接架构的特征的多尺度结构，如图所示。 3. 解码器块包含两个卷积层。 对于同样的规模，解码器块使用跳过连接密集连接。 跨尺度连接也用于连接多尺度特征。

所提出的融合网络 TransFusion 是端到端和无监督的，其框架如图 2 所示。TransFusion 由三个组件组成：编码器、融合转换器和解码器。重要符号缩写的含义显示在命名法中。需要注意的是，端到端的过程涉及颜色空间转换操作，这是医学图像融合领域中的常规数据处理。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672637208705.png" alt="1672637208705" style="zoom:80%;" />![1672663614725](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672663614725.png)

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672663640066.png" alt="1672663640066" style="zoom:80%;" />

##### Fusion Transformer

考虑到多模态的互补性质，我们的想法是通过利用Transformer对远程依赖关系的能力来整合其全局背景。 我们的融合Transformer是一个序列到序列模型，它采用一系列多模式特征向量作为输入，输出一系列 "翻译" 的多模式特征向量。

Fusion Transformer的示意图如图4所示。形式上，让单模态的特征图c Si (i = 1 and 2; c = 1, 2, 3, and 4)的维度为H×W× df, 其中 H、W 和 df 分别是特征图的高度、宽度和通道数。为了清楚起见，以下省略了c Si 的上标c。 S 模态的特征图被堆叠在一起，然后展平到维度 T × d f，其中 T = S ∗ H ∗ W。具有维度 T × d f 的可学习位置编码与展平的序列向量逐元素相加，以合并多模态的空间依赖性特征。这个结果序列向量，表示为 in ∈ RT×d f ，被送入融合转换器。

输入序列in首先被线性投影为三个矩阵：查询（Q）、键（K）和值（V）

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672640047845.png" alt="1672640047845" style="zoom:80%;" />

其中WQ ∈ Rd f × d f、WK ∈ Rd f × d f和WV ∈ Rd f × df是可学习的权重矩阵。 接下来，融合变压器计算Q和K的缩放点积，得到注意力分数，然后将分数分配给V ，公式为

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672648394499.png" alt="1672648394499" style="zoom:80%;" />

将(3)中的注意力操作拆分为h个头以并行计算，其结果如下串联：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672648431922.png" alt="1672648431922" style="zoom:80%;" />

最后将多头注意的输出送入全连通层得到输出序列，在多头关注和完全连接层中，使用层归一化和跳跃连接两者。其公式为

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672649120760.png" alt="1672649120760" style="zoom:80%;" />

out（（S * H * W）× df）然后被分割成S个模态并被重新整形为特征图S型1和S型2个尺寸为H × W × df。我们的融合变压器由L个关注层组成，即（2）-（7）中的操作循环L次。在实现中，融合变压器的架构类似于GPT [36]。

##### 损失函数

理想的融合图像应该包含来自源图像的大量信息。 为了确定融合图像中的源图像的活动级别，我们测量了MR图像S1和PET/SPECT图像S2的Y通道Y2的丰度级别。

**从信息论的角度来看，熵值越大，表示图像中包含的信息量越丰富[37]。因此，熵被用作丰度测量标准。**它通过计算每个灰度像素的概率来考虑图像中的整个像素强度分布，公式为

我们应用结构相似性指数测量（SSIM）[38]作为损失函数，可定义为

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672664569779.png" alt="1672664569779" style="zoom:80%;" />

其中X和Y是两幅图像，SSIM从三个方面评估两幅图像之间的相似性：亮度、对比度和结构。

##### 四.实验结果

为了验证拟议的TransFusion的融合性能，我们进行了两项医学图像融合任务：MR-T1和PET融合、MR-T2和SPECT融合。 还用几种代表性的图像融合方法进行定性和定量比较。 然后对消融研究进行和分析。 最后，我们测试了方法的运行时间和竞争对手。

###### 数据集和训练详情

拟议的 TransFusion 在公开可用的哈佛医学数据集上得到验证。1 对于 MR-T1 和 PET 图像融合，我们收集了 279 对 MR-T1 和 PET 图像用于训练，30 对用于测试。对于 MR-T2 和 SPECT 图像融合，我们收集了 318 对 MR-T2 和 SPECT 图像用于训练，30 对用于测试。所有原始图像的大小为 256 × 256。为了增加训练样本的数量，我们将 MR-PET（即 MR-T1 和 PET）训练图像对切割成 14577 个图像块对和 MR-SPECT（即 MR- T2 和 SPECT) 将图像对训练成 20220 个图像块对。训练图像块的大小为 64×64。

至于超参数，在 MR-T1 和 PET 融合任务中 η = 1，在 MR-T2 和 SPECT 融合任务中 η = 0.1。总训练周期为 30，学习率为 0.0001。批量大小为 32。整个网络使用 AdamW 优化器进行了优化。

##### 比较方法

我们将提出的TransFusion与八种代表性图像融合方法进行了比较，即CNN [24]、CSMCA [39]、DDcGAN [20]、U2 Fusion [18]、DSAGAN [22]、RFN-Nest [35]、EMFusion [19]和IFT [34]。

CNN [24] 首先使用 Siamese CNN 网络生成源图像的权重图，然后将源图像和权重图分别分解为拉普拉斯金字塔和高斯金字塔。接下来，分解后的系数通过基于局部相似性的自适应融合模式进行融合。最后，使用拉普拉斯金字塔重建融合图像。由于该方法可以融合解剖图像和功能图像，因此我们无需重新训练即可测试 CNN 方法。

CSMCA [38] 集成了形态成分分析和卷积 SR，以同时实现源图像的多成分和全局 SR。它执行解剖-解剖图像融合。因此，我们通过与我们相同的色彩空间转换在 MR-PET 和 MR-SPECT 融合上测试 CSMCA。

DDcGAN [20]由生成器和两个鉴别器组成。 它首先通过解卷层将不同分辨率的源图像映射到同一分辨率，然后将图像连接到通道维度并输出引信器中的图像。 两种鉴别器用于区分融合图像和两种源图像。 我们重新培训DDcGAN用于MR-PET和MR-SPECT融合。

U2Fusion [18]是一个统一的模型，可以融合MR和PET图像。因此，我们在未进行再培训的情况下测试U2Fusion。在测试期间，U2Fusion沿着通道维度连接源图像并馈送到DenseNet以获得融合图像。

DSAGAN [22]设计用于解剖-功能图像融合。因此，我们在未进行再培训的情况下测试DSAGAN。在测试过程中，DSAGAN首先利用一个带有注意力模块的双流卷积网络对两幅源图像进行特征提取，然后将源图像的特征串接输入到另一个带有4个线性层和1个注意力模块的网络中重构融合图像。

RFN-Nest [35]首先使用固定编码器从源图像中提取多尺度特征，然后使用残差块在每个尺度融合特征。融合后的特征被输入到基于嵌套连接的解码器中以生成融合图像。由于RFN-Nest是为红外和可见光图像融合而开发的，我们通过与我们相同的颜色空间转换重新实现了用于MR-PET和MR-SPECT融合的RFN-Nest。

EMFusion[19]：沿着通道维度连接源图像并馈送到DenseNet以生成融合图像。在训练过程中，它通过表层和深层约束来约束来自源图像的保留信息。我们针对MR-PET和MR-SPECT融合重新训练EMFusion。

IFT [34]首先使用固定编码器从源图像中提取多尺度特征，然后使用由并行卷积分支和变换分支组成的融合模块在每个尺度上融合特征。融合后的特征被输入到基于嵌套连接的解码器中以生成融合图像。我们针对MR-PET和MR-SPECT融合重新训练IFT。

##### 结果

文中给出了两种医学图像融合任务的定性和定量比较结果。定性结果可以直观地评价融合效果，定量结果可以客观地评价图像质量。**定量评价主要有四组度量标准，分别从信息论、结构相似性、图像特征和人类感知的角度对图像进行评价。**在本文中，我们从每个组中选择一个或多个指标，以确保客观和全面的评估。总共使用八个度量，包括基于信息论的互信息（MI）和峰值信噪比（PSNR）;基于结构相似性的SSIM、平均梯度（AG）、边缘强度（EI）、标准差（SD）和基于图像特征的基于边缘的相似性度量（QAB/F）[40];以及基于人类感知的QCV [41]。MI测量从源图像传输到融合图像的信息量。PSNR度量融合图像与源图像之间的失真。SSIM评价融合图像与源图像的结构相似性。AG能够反映图像的纹理和细节，用于度量融合图像中的梯度信息。EI测量融合图像中的边缘强度信息。SD表示融合图像的对比度。QAB/F测量从源图像转移到融合图像的边缘信息量。QCV比较融合图像和源图像之间的视觉差异。原始MR图像是灰度图像，PET/SPECT图像是RGB图像。它们的融合图像在RGB颜色空间中。以MI和SSIM度量为例，灰度图像和RGB图像之间的度量计算可以公式化如下：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672665503868.png" alt="1672665503868" style="zoom:80%;" /><img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672665528454.png" alt="1672665528454" style="zoom:80%;" />

其中S(R/G/B) f 和S(R/G/B) 2 分别代表S f 和S2的R/G/B通道。具体地，首先用MR图像S1和PET/SPECT图像S2的对应通道测量融合图像S f 的每个通道。然后，可以通过对 R、G 和 B 三个通道进行平均来获得最终度量。为简洁起见，所有使用的度量的数学定义和详细计算过程可以在[42]中看到。

1) MR-T1 和 PET 图像融合：我们在 30 个 MR-T1 和 PET 图像对上测试和比较了所提出的 TransFusion。图 5 显示了三个典型的定性比较示例。在补充材料中可以看到 30 个图像对的完整定性比较结果。相比之下，我们的融合图像具有三个代表性优势。**第一个是我们的融合图像显示了 MR-T1 图像中的纹理细节和 PET 图像中的功能信息的出色保留，**而在八种比较方法的融合图像中，纹理细节模糊甚至丢失到不同的程度，如图5第一行所示。此外，DDcGAN和DSAGAN融合图像中的像素强度较暗，表明这两种方法不能很好地保留PET图像中的颜色（功能）信息。更糟糕的是，CNN 结果的功能信息丢失严重。其次，**我们的融合图像可以比 CNN、CSMCA、DDcGAN、U2Fusion、DSAGAN、EMFusion 和 IFT 更好地减轻 PET 图像的马赛克**，如图 5 的第二行所示。**第三，我们的融合图像增强和细化边缘通过考虑 MR-T1 图像中的纹理细节**，在 PET 图像中，如图 5 的第三行所示。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672665865701.png" alt="1672665865701" style="zoom:80%;" />

表I报告了MR-T1和PET图像融合的定量结果。在MI、AG、EI、QAB/F和QCV指标方面，我们的TransFusion实现了最佳结果，优于其他竞争对手。在PSNR、SD和SSIM指标上，我们的TransFusion实现了次优结果，但与最优结果相当。PSNR和SSIM的次优值可能是由于以下原因。如上所述，我们的TransFusion可以细化PET图像中的彩色边缘。因此，不可避免地会稍微降低融合图像和PET图像的相似度。然而，诸如QAB/F（测量传送的边缘信息的量）和QCV（基于人的感知测量图像质量）的其它度量可以被提升。与CNN和CSMCA相比，申报的TransFusion在AG、EI和SD指标方面略上级或相当，在MI、SSIM、QAB/F和QCV指标方面远远优于CNN和CSMCA，分别获得了53.19%和54.59%、7.31%和4.71%、9.26%和11.65%以及66.15%和66.26%的改善。与DDcGAN、U2 Fusion、DSAGAN、RFN-Nest和IFT相比，申报的TransFusion在所有指标上均优于它们。即使使用这些比较方法中每个指标的最佳值，我们的TransFusion仍具有13.63% MI、10.51%AG、8.69% EI、5.02% SD、9.76% QAB/F、12.68% SSIM和34.20% QCV的增益。对于EMFusion，**我们的TransFusion在PSNR和SSIM上相当，但比其他六个指标具有显著优势。**总之，与其他竞争产品相比，申报的TransFusion能够更好地实现MR-T1和PET图像融合。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672665978616.png" alt="1672665978616" style="zoom:80%;" />

MR-T2和SPECT图像融合：我们在30对MR-T2和SPECT图像上测试并比较了申报的TransFusion。图6显示了三个定性比较结果。30个图像对的完整对比结果可参见补充材料。由于MR-SPECT融合任务与MR-PET任务相似，因此其结果显示出相似的成像风格。因此，在本节中进行了简要分析。一方面，我们的融合图像很好地保留了MR-T2图像中的纹理，同时细化了SPECT图像中的功能信息，如图6的第一行和第三行所示。另一方面，我们的融合图像减轻了SPECT图像中的马赛克，如图6的第二行所示。



表I显示了MR-T2和SPECT图像融合的定量结果。该算法在MI、QAB/F、QCV等指标上取得了最优结果，在PSNR、AG、EI、SD、SSIM等指标上取得了次优结果，具有上级的融合性能。尽管DSAGAN在AG、EI和SD这三个基于图像特征的度量上获得了第一名，但它在其他度量上较弱，排名低于第六名。与EMFusion相比，我们的结果在PSNR和SSIM上略低，但在其他六个指标上有显著优势，分别获得了26.11%的MI、26.58%的AG、26.24%的EI、15.85%的SD、12.10%的QAB/F和49.52%的QCV。与CNN、CSMCA、DDcGAN、U2 Fusion、RFN-Nest和IFT相比，所提出的TransFusion在所有指标上都优于这6种方法。考虑到所有8个指标，与其他竞争产品相比，申报的TransFusion能够更好地实现MR-T2和SPECT图像融合。

#####消融实验

带/不带Fusion Transformer：为了验证融合<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672666074285.png" alt="1672666074285" style="zoom:80%;" />的有效性，我们在所有尺度上去除了融合Transformer。在这种情况下，**普通求和运算是融合策略，而不是Transformer。**MR-PET和MR-SPECT融合任务的定量对比结果如表II所示。对于这两种融合任务，有Transforme的融合结果总体上优于无Transforme的融合结果。所提出的融合Transforme在全局理解中融合解剖和功能图像的特征。该算法通过考虑图像的长程语义信息，可以细化PET/SPECT图像的彩色边缘。然而，有变换器的融合结果比没有变换器的融合结果具有更低的SSIM度量。可能是由于以下原因造成的。如在第IV-C节中提到的，细化颜色边缘不可避免地带来融合图像和PET/SPECT图像之间的稍微较低的相似性。值得注意的是，由于边缘的细化，诸如EI、QAB/F和QCV的度量可以被提升。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672666074285.png" alt="1672666074285" style="zoom:80%;" />

2) Number of Layers in Transformer：为了探索融合transformer中层数的影响，我们将层数设置在范围[1, 2, 3, 4, 5, 6]。图 7 和表 III 显示了不同层在 MR-PET 和 MR-SPECT 融合任务上的度量值。总体而言，随着变压器层数的增加，所有指标的值仍然相对稳定，特别是对于 MR-T1 和 PET 融合，如图 7 所示。**结果表明一层融合变压器足以融合多模态本文中使用的 MR 和 PET/SPECT 图像融合的特征。**

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672666260456.png" alt="1672666260456" style="zoom:80%;" />

3) 互动/共享模式分支：为了验证互动模式分支的有效性，我们设置要共享的权重。 MR-PET 和 MR-SPECT 图像融合的定量比较结果显示表四。 对于MR-PET和MR-SPECT图像融合，与交互模式分支的结果明显优于除SD和SSIM之外所有指标共享权重的结果。 对于多模式图像融合，互动模式分支的设计比权重共享设置更具优势。 图像融合的输入是具有不同模式的两张图像。 交互式设计可以增强两种图像模式之间的通信，促进综合特征的提取。 在MI、QAB/F和QCV指标方面，互动模式分支的设计分别分别获得22.54%/24.96%、8.32%/12.39%和27.69%/62.22%的MR-PET/SPECT融合任务。 它反映了我们的互动设计具有很高的能力，可以捕获互补和全面的特性，适合于多模式图像融合。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672666314283.png" alt="1672666314283" style="zoom:80%;" />

##### 超参数分析

我们使用温度参数来控制源图像的活动级别对重要性权重的敏感度。 当小于 1 时，重要性权重对活动级别将更加敏感。 当大于 1 时，重要性权重更不敏感。 在这里，我们探讨温度参数对融合效应的影响。 设置为范围 [0.01， 0.1 ， 1， 10， 20]。 图 8 显示了 MR -PET 和 MR-SPECT 图像融合的不同值的融合结果。 对于 MR-T1 和 PET 图像融合， 0.01 0.01 ， 0.1 和 1 时，融合效应处于同等水平。 虽然大于 1 ，但融合图像的纹理细节逐渐模糊甚至丢失，如图 8中的第一行所示。 MR-T2 和 SPECT 图像融合出现同样的现象，只有 0.1 的阈值，即当大于 0.1 时，融合效应越来越差，可以从图 8 的第二行观察到。 基于熵来自适应地设置解剖学和功能图像的活动水平，而不是固定活动水平。 较大的熵值表示图像中包含的信息量更丰富。 与固定活动水平设置相比，我们的自适应测量可以通过考虑源图像的信息量来更灵活地融合多模式图像。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672666746724.png" alt="1672666746724" style="zoom:80%;" />

它带来了解剖学和功能图像中重要信息的优越保存的优势。 这一点在图8中得到了证实。 由于活动水平越来越不受信息量的影响，解剖图像中的纹理细节越受损。

##### 运行时间

为了验证该方法的有效性，我们在30对图像上测试了该方法和8种比较方法的平均运行时间。结果如表V所示。在这些方法中，CNN和CSMCA是在配备2.40 GHz英特尔酷睿i5 CPU的计算机上测试的。DDcGAN、U2 Fusion、DSAGAN、RFN-Nest、EMFusion和我们的TransFusion在八核CPU或NVIDIA GeForce GTX 2080 Ti GPU上进行了测试。从表V可以看出，我们的方法对于融合一对图像是时间高效的，证明了其在医学图像融合任务上的高效性。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672666447571.png" alt="1672666447571" style="zoom:80%;" />

##### 五、结论

本文提出了一种基于变压器的端到端融合框架，用于多模式医学图像融合，称为TransFusion。 我们的TransFusion引入了Transformer结构，作为融合多式联运功能的全局背景的融合策略。 此外，在多个比例下通过聚变器相互作用的分支网络被设计来充分融合多式联运特征。 我们开发了结构相似性损耗，以最小化源图像和融合输出之间的距离，并采用饱和度测量源图像的信息保存权重。 我们的融合设计带来了以下好处：由此产生的融合图像很好地保留了MR图像中的纹理，并增强了PET/SPECT图像中的功能信息。 我们验证了MR-PET和MR-SPECT图像融合任务的拟议方法。 定性和定量比较结果表明，我们的方法优于最先进的融合方法。

         粒计算的基本组成主要包括 3 部分:粒⼦、粒层和粒结构。粒⼦是构成粒计算模型的最基本元素[ 58-59], 是粒计算模型的原语.⼀个粒可以被解释为许多⼩颗粒构成的⼀个⼤个体 ,现实⽣活中, 粒⼦⽆处不在 ,如在地图上观察洲 、国家 、海洋 、⼤陆和⼭脉等是⼀些粗的粒⼦(⼤的粒⼦),观察省、市 、区等是⼀些中等的粒⼦, ⽽观察街道 、饭店 、机场等是⼀些相对较⼩的粒⼦.⼀个粒⼦可以被同时看作是由内部属性描述的个体元素的集合, 以及由它的外部属性所描述的整体.⼀个粒⼦的存在仅仅在⼀个特定的环境中才有意义.⼀个粒⼦的元素可以是粒⼦,⼀个粒⼦也可以是另外⼀个粒⼦的元素.⽽衡量粒⼦“⼤⼩”的概念是粒度,⼀般来讲, 对粒⼦进⾏“量化”时⽤粒度来反映粒化的程度  

⼀个粒化准则对应⼀个粒层 ,不同的粒化准则对应多个粒层, 它反应了⼈们从不同⻆度、不同侧⾯来观察问题 、理解问题 、求解问题.所有粒层之间的相互联系构成⼀个关系结构 , 称为粒结构.粒结构给出了⼀个系统或者问题的结构化描述.通过从系统思维 、复杂系统理论和层次结构理论(技术)中得到的启发⾄少需要确定⼀个粒结构⽹中 3 个层次的结构:粒⼦的内部结构 、粒⼦集结构和粒⼦⽹的层次结构.粒⼦集的集体结构可以看作是全部层次结构中⼀个层次或者⼀个粒度视图中的结构.它本身可以看作是粒的内部连接⽹络.对于同⼀个系统或者同⼀个问题, 许多解释和描述可能是同时存在的.所以 ,粒结构需要被模型化为多种层次结构 ,以及在⼀个层次结构中的不同层次.虽然⼀个粒⼦在某个粒层上被视为⼀个整体,但粒⼦内部元素(⼦粒⼦)的结构在问题求解时也很重要, 因为它能提供粒⼦更为详细的特性.⽽在同⼀层上的粒⼦之间也具有某种特殊的结构, 它们可能是相互独⽴ ,或者部分包含.如果同⼀粒层上的粒⼦之间的独⽴性越好,可能问题求解后合并起来越⽅便;反之 ,如果粒⼦之间的相关性越好, 则问题求解后的合并⼯作相对越繁杂.粒⼦⽹的层次结构是对整个问题空间的概括 ,它的复杂性在⼀定程度上决定了问题求解的复杂程度.