### IMAGE FUSION TRANSFORMER

**摘要：**在图像融合中，将从不同传感器获得的图像融合以生成具有增强信息的单个图像。近年来，最先进的方法采用卷积神经网络 (CNN) 来编码图像融合的有意义的特征。具体来说，基于 CNN 的方法通过融合局部特征来执行图像融合。==但是，他们不考虑图像中存在的远程依赖性。基于 Transformer 的模型旨在通过在自注意力机制的帮助下对远程依赖项进行建模来克服这一问题。==这促使我们提出一种新颖的 Image Fusion Transformer (IFT)，我们在其中开发了一种基于 Transformer 的多尺度融合策略，该策略同时处理本地和远程信息（或全局上下文）。所提出的方法遵循两阶段训练方法。在第一阶段，我们训练一个自动编码器来提取多尺度的深度特征。在第二阶段，使用 SpatioTransformer (ST) 融合策略融合多尺度特征。 ST 融合块由 CNN 和分别捕获局部和远程特征的变换器分支组成。在多个基准数据集上进行的大量实验表明，所提出的方法比许多竞争性融合算法表现更好。此外，我们通过消融分析展示了所提出的 ST 融合策略的有效性。

#### 一、简介

图像融合已被证明在许多现实世界的应用中至关重要，例如军事 [1]、计算机视觉 [2]、遥感 [3] 和医学成像 [4]。它是指将同一场景的不同图像组合起来，整合互补信息，生成单一的融合图像。例如，使用可见光传感器拍摄的图像具有丰富的精细细节，如颜色、对比度和纹理。然而，可见光传感器在光线不足的情况下无法区分物体和背景。使用热传感器获得的图像捕捉了白天和夜间将物体与背景区分开来的显着特征。然而，热传感器缺乏关于物体的纹理和颜色空间信息。这是因为可见光传感器的工作波长为 300-530 µm，而热传感器的工作波长为 8-14 µm [5]。拥有包含来自可见光和热传感器的补充信息的单个图像将非常有用。图像融合专门通过融合来自不同来源的互补属性来生成详细的场景表示来解决这个问题 [6]。

传统的图像融合方法包括基于稀疏表示 (SR) 的方法 [8, 9]；基于多尺度变换的方法 [10, 11]；基于显着性的方法[12]和基于低秩表示（LRR）的方法[13]。即使这些方法取得了有竞争力的性能，也存在一些缺点：1）它们的泛化能力较差，因为它们依赖于手工特征提取，2）SR 和 LRR 中的字典学习非常耗时 [7]，以及 3）不同的源集图像需要不同的融合策略。

最近的图像融合工作探索了基于 CNN 的融合技术，[7]，[14]，它通过克服上述缺点优于传统技术。尽管现有的基于 CNN 的融合技术通过学习局部特征提高了泛化能力，但它们无法提取图像中的远程依赖关系。这会导致丢失一些可能对示例性融合图像有用的基本全局上下文。因此，我们认为将局部特征与远程依赖相结合可以添加全局上下文信息，这反过来有助于进一步提高融合性能。出于这个动机，我们提出了一种图像融合变换器（IFT），它具有一种新颖的空间变换器（ST）融合策略，可以有效地学习多个尺度的局部特征和远程信息，以融合来自给定图像的互补信息（见图1).这项工作的主要贡献可以概括如下：

* 我们提出了一种称为 Image Fusion Transformer (IFT) 的新型融合方法，它利用局部信息和模型远程依赖性来克服近期图像融合工作中存在的缺乏全局上下文理解的问题。
* 所提出的方法利用了一种新颖的 Spatio-Transformer (ST) 融合策略，其中采用空间 CNN 分支和 Transformer 分支来利用局部和全局特征更好地融合给定图像。
* 所提出的方法在多个融合基准数据集上进行了评估，与现有的融合方法相比，我们取得了有竞争力的结果。

#### 2. 相关工作

##### 2.1.图像融合

传统的图像融合方法采用离散余弦变换（DCT）[15]、稀疏表示（SR）[8]、主成分分析（PCA）[16]等来提取有用的特征。然而，这些特征提取方法缺乏通用性。此外，从不同来源捕获的图像需要不同的融合策略。因此，传统的融合策略是以源特定的方式设计的。

为了克服传统方法的这些问题，引入了基于深度学习的图像融合方法。 Li [17] 提出了一种首先分解可见光和热图像的技术。随后，他们通过平均特征融合和基于深度学习的特征融合使用分解后的图像进行融合。 Li [18] 提出的另一项工作使用完全基于卷积的模型来融合可见光和热图像。在这里，使用 DenseNet 编码器提取源图像的特征，并使用基于 CNN 的融合层进行融合。然后使用基于 CNN 的解码器来获取融合图像。 Li [7] 进一步将他们之前的工作 [18] 扩展到多尺度深度特征的端到端融合策略，最大限度地减少建议的细节和特征损失。 Xu [14] 提出了一个统一的无监督端到端框架，通过将融合问题与持续学习相结合来解决融合问题。然而，所有这些方法都侧重于学习源图像之间的空间局部特征，而没有考虑源图像中存在的远程依赖性。在这项工作中，我们探索除了局部特征之外还提取远程特征，以进一步提高融合质量。

##### 2.2.Transformer

Transformer 模型架构首先由 Vaswani [19] 提出，多年来已被证明在自然语言处理 (NLP) 文献中极为重要。与递归神经网络和 CNN 相比，基于 Transformer 的模型的成功可以归因于它们能够捕获更好的远程信息。受他们成功的激励，Dosovitskiy 提出了一种用于图像分类的视觉转换器 (ViT) [20]。这激发了人们对开发基于变换器的视觉问题方法（如对象检测 [21] 和分割 [22]）的浓厚兴趣。因此，在这项工作中，我们还利用基于 transformer 的架构，通过使模型能够对图像的远程依赖性进行编码来获得改进的图像融合性能。

#### 3. 提出的方法

##### 3.1.图像融合变压器（IFT）

所提出的 Image Fusion Transformer (IFT) 是一种融合网络，它接收输入源图像并生成增强的融合图像。 IFT 由三部分组成：编码器网络、SpatioTransformer (ST) 融合网络和嵌套解码器网络，如图 2 所示。编码器网络由四个编码器块组成，其中每个编码器块包含一个内核大小为 3×3 的卷积层其次是 ReLU 和最大池化操作。对于给定的源输入，我们从编码器网络的每个卷积块中提取多尺度的深度特征。然后使用 ST 融合网络在多个尺度上融合从两个图像中提取的这些特征。 ST融合网络由一个空间分支和一个transformer分支组成。空间分支由转换层和瓶颈层组成，用于捕获局部特征。 transformer 分支由一个基于轴向注意力的 transformer 块组成，以捕获远程依赖项（或全局上下文）。最后，我们通过以融合特征作为输入训练嵌套解码器网络来获得融合图像。解码器网络基于 RFN-Nest [7] 架构。

##### 3.2.自我注意和轴向注意

自注意力是一种注意力机制，它关联单个序列的不同标记，以便计算同一序列的表示。令x∈RCin×H×W和y∈RCout×H×W为输入和输出特征，其中Cin和Cout分别是输入和输出通道的数量，H和W分别对应于高度和宽度。输出 y 计算如下：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672799821408.png" alt="1672799821408" style="zoom:80%;" />

其中 qij 、 kij 和 vij 是任意位置 i ∈ {1, ..,H} 和 j ∈ {1, ..,W} 的查询、键和值，计算为 q = WQx, k = WKx 和 v = WVx，分别。从等式。 1，我们可以推断，self-attention 计算整个特征图的远程亲和力，这与 CNN 不同。然而，由于其二次复杂性，这种自我注意机制在计算上是昂贵的。

因此，我们使用轴心注意机制[23]，计算效率更高。 具体来说，在轴向注意中，自我提取首先在特征图高轴上进行，然后在宽轴上进行，从而降低计算复杂性。 此外，王[24]提出了一个可学习的位置嵌入到轴向注意力查询，键和值，使亲和力对位置信息敏感。 这些位置嵌入是训练过程中联合学习的参数。 因此，对于给定的输入x，沿高轴的自我注意可以计算为：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672799873118.png" alt="1672799873118" style="zoom:80%;" />

其中 rq, rk, rv ∈ RH×H 是高度轴的位置嵌入。对于轴向注意力，我们计算方程式。 2 沿着高度和宽度轴，它提供了一个有效的自注意力模型。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672799894229.png" alt="1672799894229" style="zoom:80%;" />

Spatio-Transformer (ST) 融合机制的前馈路径。来自图像 1 和图像 2 的编码特征图被馈送到空间分支和变换器分支。空间分支提取精细的局部特征，而变换器分支提取远程特征。

#####3.3. Spatio-Transformer (ST) 融合策略

拟议的 ST 融合块由两个分支组成：空间分支和变换器分支。在空间分支中，我们使用一个转换块和一个瓶颈层来捕获局部特征。在 Transformer 分支中，我们使用轴向注意力通过自注意力机制对远程依赖建模来学习全局上下文特征。我们添加这两个特征以获得包含增强的局部和全局上下文信息的融合特征图。此外，我们在多个尺度上应用我们的 ST 融合策略，然后将其转发给解码器网络以获得最终的融合图像。 ST 融合块如图 3 所示。

##### 3.4.损失函数

所提出的方法经过训练以保留精细的结构细节并保留突出的前景和背景细节。训练 IFT 的总体训练目标，表示为 Lfuse，可以表示为：<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672800411161.png" alt="1672800411161" style="zoom:80%;" />

其中 Ldet 是结构相似性损失，计算如下<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672800454452.png" alt="1672800454452" style="zoom:80%;" />

其中 O 和 I 分别是融合的和输入的源图像。此外，SSIM(.) 衡量结构相似性。如果 SSIM(O, I) 趋于 1，则融合后的图像保留了源图像的大部分结构细节，反之亦然。特征相似性损失Lfeat计算如下

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1672800498336.png" alt="1672800498336" style="zoom:80%;" />

其中 M 是提取深层特征的数量级； f、I1、I2分别表示融合图像、输入源1图像和输入源2图像。此外，w1、wI1、wI2 是平衡损失大小的权衡参数。 Φm f 是融合特征图，而 ΦI1 和 ΦI2 分别对应输入源 1 和输入源 2 图像的编码特征图。这种损失限制了融合的深层特征以保留显着结构，从而增强了融合特征空间以学习更多显着特征并保留精细细节。这里，α是一个超参数。