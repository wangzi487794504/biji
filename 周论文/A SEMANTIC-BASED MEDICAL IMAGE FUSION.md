# A SEMANTIC-BASED MEDICAL IMAGE FUSION

**摘要：**临床医生需要综合分析不同来源的患者信息。医学图像融合是一种很有前途的方法，可以从不同模式的医学图像中提供整体信息。然而，现有的医学图像融合方法忽略了图像的语义，使得融合图像难以理解。在这项工作中，我们提出了一种新的评估指标来衡量融合图像的语义损失，并提出了一种用于多模态医学图像融合的 Fusion W-Net (FW-Net)。实验结果很有希望：我们的方法生成的融合图像大大减少了语义信息的损失，并且与五种最先进的方法相比具有更好的视觉效果。我们的方法和工具具有在临床环境中应用的巨大潜力。

## 一.介绍

不同形态的医学图像提供不同类型的信息，在临床诊断中发挥着越来越重要的作用。例如，计算机断层扫描 (CT) 图像显示密集结构（如骨骼和植入物）的信息，而磁共振 (MR) 图像显示高分辨率解剖信息，如软组织 [1]。通常，临床医生必须彻底研究不同形式的医学图像，以便为每位患者提供准确的诊断。该行业正致力于开发具有混合成像技术的设备，用于直接获取图像，例如 MR/PET 和 SPECT/CT [2, 3]。然而，这些设备不仅非常昂贵，而且难以获得任意两种模态的混合医学图像。幸运的是，还有另一种低成本的选择：对于每个患者，我们可以融合现有的不同模态的医学图像，即 CT 和 MR-T2（T2 加权）图像。这种替代方案很容易推广，因为它能够以最小的信息损失融合任何模态医学图像[4]。

详细地说，这些低成本的方法如下: 在变换域 [5,7，8,6，1] 的基础上，首先，它们将源图像转换成不同比例的特定系数，然后根据几个手工制作的规则对系数进行融合，最后将系数反转成融合图像。

不幸的是，在不同方式的医学图像中存在语义冲突，这被以前的方法所忽略。==这里的语义概念是指不同模态的医学图像中的亮度代表不同的含义。==例如，CT图像的亮度表示组织的密度，而MR-T2图像的亮度表示组织的流动性和磁性。因此，不同源图像中亮度的语义完全不同。如果不解决语义冲突，则融合的图像很难阅读，因此在临床环境中无用。具体而言，这些方法的两个主要缺点如下 :( 1) 现有方法忽略了不同源图像的语义冲突，这将导致融合图像中严重的语义损失。在图1中，蓝色箭头指向高密度，低流动的头骨，**红色箭头指向低密度，高流动的脑脊液 (在心室中)。源图像 (a) 和 (b) 中的亮度语义明显不同。**但是，在融合结果 (c) (e) 中，颅骨 (蓝色箭头) 和脑脊液 (红色箭头) 的亮度没有差异。(2) 不考虑亮度语义的融合方法会导致某些脑组织边界模糊。在图1 (b) 的绿色框架中，我们可以清楚地看到额窦的炎症区域，这是临床医生关注的重点。但是，由于图1 (a) 中的相应部分是明亮的，因此融合结果 (c) - (e) 中的额窦边界变得模糊。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1667706058416.png" alt="1667706058416" style="zoom:80%;" />

在本文中，受上述问题的启发，我们首先提出了一种基于语义的融合方法。我们提供了一个基于自动编码器的框架，我们将其命名为 Fusion W-Net (FW-Net)。 FW-Net在融合图像中尽可能编码从源图像序列中提取的各种信息，我们提出的语义损失结合[9]中的结构损失可以有效地将信息组织成视觉融合图像。这项工作并不是第一个将 U-Net [10] 和自动编码器框架结合起来的工作。 WNet [11] 被提出用于图像分割任务。然而，我们的 FW-Net 在损失函数和网络结构方面与它不同。在本文中，我们专注于 CT 和 MR-T2 的医学图像融合。然而，我们的方法可以推广到其他图像。我们的贡献如下：

1) 我们揭示了当前图像融合方法难以在临床环境中应用的原因，即语义冲突被忽略了。

2) 我们提出了一种新颖的fw-net模型来融合不同模态的医学图像。

3) 提出了一种度量来评估融合图像中的语义损失，并将其用作损失函数的一部分。

## 二.提出的方法

所提出模型的总体框架如图 2 所示。将一对注册图像 xct 和 xmr 堆叠并馈送到编码器 Eθ，以生成与源图像大小相同的融合图像 y。然后解码器Dφ获取融合图像并生成两个重建图像^xct和^xmr。该框架是一个无监督的端到端模型。我们的总体目标是

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1667707907650.png" alt="1667707907650" style="zoom:80%;" />

其中x表示源图像序列，x表示重建图像序列，y表示融合图像。L~reconstruct~是从像素级别测量源图像和重建图像之间的差异。L~SL~和L~MEF_SSIM~从补丁级测量源图像与融合图像之间的语义和结构损失。重建损失的计算公式为

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1667715059774.png" alt="1667715059774" style="zoom:80%;" />

其中 ||·|| 为图像的 L2范数，x(i) 为第i个训练例，N为训练样本数。通过最小化重建图像和源图像之间的均方误差 (MSE)，编码器将源图像的结构信息，纹理和更重要的语义保留到融合图像中。然而，编码器生成的融合图像并不局限于视觉上相当可观。我们添加了额外的LSL和LMEF SSIM来约束融合图像的语义和结构，从而使融合图像在视觉上令人印象深刻。消融研究在第3.3节中。以下小节详细介绍了损失函数和网络体系结构。

### 2.1语义损失

在第1节中，我们提到亮度指示图像中组织的属性。例如，在CT图像中，亮度表示组织的密度，而在MR图像中，亮度表示组织的流动性和磁性。在融合图像中，由于融合图像和源图像的像素值范围相同，因此我们需要对融合图像中的亮度赋予更多的含义。因此，我们提出了一种评估指标来评估融合图像中的语义损失 (SL)。令 {xk} = {xk | k = 1,2} 表示从源图像序列x中的相同空间位置提取的图像块的集合，令y为融合图像y中的对应块。给定源图像序列x和融合图像y，我们定义医学图像的语义评价指标如下:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1667715721122.png" alt="1667715721122" style="zoom:80%;" />

其中m是图像中的补丁数， x^i^~k~和y^i^是x和y中的第i个补丁，µ xi和 µ~y~^i^分别是xi k和yi的平均值。C用于求均值，值为 (M +1)*M /2， x和y均归一化为 [0,1] 区间。

|μ~x~i~k~ -  µxj |表示在一种模态下两个 k 块之间的亮度差异，而 |µ~y^i^~ - µ~y^j^~ |表示融合图像中相应块的亮度差异。这两个术语之间的差异表明一种模态的语义变化在融合图像中是否一致。考虑到融合图像的语义是所有源图像的组合，我们取融合补丁与不同模态补丁之间的最大语义差异来表示融合补丁的语义损失。我们的语义损失枚举了所有补丁组合以获得最终结果。值得注意的是，我们已经从计算中删除了背景补丁，因为它们不包含任何语义信息。较低的 SL 表示融合图像的语义损失较低。我们的语义损失如下：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1667717145840.png" alt="1667717145840" style="zoom:80%;" />

对于多通道图像，我们需要将它们转换为YCbCr颜色通道数据，然后在亮度通道中进行比较。这是由于亮度通道的亮度变化比其他通道更明显。

### 2.2MEF SSIM 损失

多曝光图像融合 (MEF) 被认为是一种有效的质量增强技术，广泛应用于电子产品中 [12]。MEF将一系列具有不同曝光水平的图像作为输入，并合成具有更多信息的图像 [13,14]。[9] 提出了MEF结构相似性指数 (MEF SSIM) 来评估不同的MEF算法。之后，Prabhakar [15] 提出了一个最先进的模型，该模型以MEF SSIM为损失函数，用于MEF任务。受 [15] 的启发，我们将MEF SSIM用作损失函数的一部分，期望保留源图像的结构和更清晰的部分。SSIM [16] 框架将patch分为三个部分: 结构 (s)，亮度 (l) 和对比度 (c)。通过以下方式将给定的图像补丁分解为三个分量

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1667717322367.png" alt="1667717322367" style="zoom:80%;" />

uxk是补丁的平均值，并且x~k是均值去除的补丁。由于更高的对比度值意味着更好的图像，因此融合图像块 ^c 的所需对比度由下式计算：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1667721439910.png" alt="1667721439910" style="zoom:80%;" />

[0015] 对于结构信息，考虑到源图像中对应的位置结构不同，融合图像补丁的期望结构为源图像补丁的组合，可以通过以下公式得到:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1667722740172.png" alt="1667722740172" style="zoom:80%;" />

### 2.3网络架构

我们的编码器E~θ~ 和解码器d~φ~ 的基本框架遵循U-Net的结构 [10]。U-Net是全卷积网络 (FCN) [17]，用于医学图像分割。它将第i层的特征图复制到第n-i层，其中n是层的总数。==网络的低级特征图保留了图像的细粒度信息，而高级特征图保留了图像的高级语义信息和高频部分。==它有利于医学图像融合任务，因此我们在编码器和解码器中使用U-Net。**3 × 3卷积的步长为1，填充为1。**所以在每次卷积操作之后，特征图的大小都不会改变。解码器的结构几乎与编码器的结构相同，只是输入尺寸为1 × 256，输出尺寸为2 × 256。**我们用双线性插值运算代替反卷积运算。**尽管反卷积操作可以增加模型的容量，但它使编码器生成的融合图像的质量较差。反卷积操作会在融合图像中产生明显的胡椒噪声和模糊，而双线性插值操作会产生更清晰，更平滑的图像。

## 三.实验

### 3.1实验设置

在我们的 FW-net 中，α 和 β 分别为 0.005 和 1。在语义损失计算中，patch size为5×5，步幅为3。在MEF SSIM loss计算中，patch size为7×7，步幅为1，C为9×10−4。批量大小设置为 1。优化器是 Adam [18]，其中学习率为 0.001。我们的 FW-Net 在 pytorch 框架中实现，并在 Tesla M40 GPU 上运行。		

我们将我们的方法与五种主流算法进行了比较，包括基于引导滤波 (GF) 的方法 [7]，非下采样contourlet (NSCT) 域中的模糊自适应简化脉冲耦合神经网络 (NSCT-RPCNN) 方法 [6]，NSCT域中的相位一致性和指向性对比 (NSCTPCDC) 方法 [8]，拉普拉斯金字塔域中的卷积神经网络 (lp-cnn) 方法 [5]，以及非下采样shearlet域中的参数自适应脉冲耦合神经网络 (nsst-papcnn) 方法 [1]。所有这些方法的参数都从提供的代码中设置为默认值。

我们在 [19] 中获得了CT和MR-T2的医学图像。图像来自十人，每人13片，共130对图像。所有源图像具有相同的256 × 256像素，其中每对CT和MR-T2图像对齐和配准。我们使用了7个人的91张图像作为训练集，2个人的26张图像作为验证集，1个人的13张图像作为测试一下集。