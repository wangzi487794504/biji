# U2Fusion: 一个统一的无监督图像融合网络 

**摘要：**本研究提出了一种新颖的、统一的、无监督的端到端图像融合网络，称为U2Fusion。它能够解决不同的融合问题，包括多模态，多曝光和多焦点情况。U2Fusion利用特征提取和信息测量，自动估计相应源图像的重要性，并得出自适应的信息保存度。因此，不同的融合任务统一在同一框架中。基于自适应度，训练网络以保持融合结果和源图像之间的自适应相似性。因此，大大减轻了将深度学习应用于图像融合的绊脚石，例如对标注数据和专门设计的度量的要求。通过避免在针对不同任务顺序训练单个模型时失去先前的融合能力，我们获得了适用于多个融合任务的统一模型。此外，发布了新的对齐红外和可见图像数据集RoadScene (可在https://github.com/hanna-xu/RoadScene上获得)，以提供基准评估的新选项。对三种典型图像融合任务的定性和定量实验结果验证了u2融合的有效性和普遍性。我们的代码可在https://github.com/hanna-xu/U2Fusion公开获得。

### 一.介绍

图像融合的应用范围很广，从安全到工业和民用领域都有。由于硬件设备或光学成像的限制，使用一种类型的传感器或单个拍摄设置捕获的图像只能捕获一部分信息。例如，亮度在有限范围内且在预定义的景深内的反射照明信息是不完整信息的典型表示。图像融合的目标是通过整合来自使用不同传感器或光学设置捕获的多个源图像的互补信息来生成合成图像。图1示出了不同图像融合任务的示意图。具有优越的场景表示和更好的视觉感知的单个融合图像适用于后续的视觉任务，如视频监控、场景理解和目标识别等。

通常，图像融合对多模态、多曝光或多焦点图像进行操作。为了解决这些问题，已经开发了大量的算法。它们可以大致分为两类: 基于传统融合框架的那些和基于端到端模型的那些。尽管这些算法在各自的融合任务中取得了可喜的成果，但仍有一些问题有待解决。==在基于传统融合框架的方法中，融合规则的有限选择和手动设计的复杂性限制了性能的提高。==在端到端模型中，==通过依靠标注数据进行有监督学习或专门设计的无监督学习指标来解决融合问题。但是，不存在用于多个任务的通用地面真实标注数据(ground truth)或无参考度量。==这些问题构成了模型统一以及监督或无监督学习应用的主要绊脚石。

同时，不同的融合任务通常具有相似的目标，即通过整合来自多个源图像的重要和互补信息来合成图像。然而，在不同的任务中，由于源图像具有不同的类型，要集成的重要信息在很大程度上是不同的 (参见第3.1节中的详细解释)，因此限制了大多数方法对特定任务的有效性。神经网络具有强大的特征表示能力，可以以统一的方式表示各种信息。它有可能导致一个统一的融合框架，这将在本研究中进行探索。

此外，通过在一个统一的模型中解决不同的融合问题，这些任务可以相互促进。例如，鉴于统一模型已经针对多曝光图像融合进行了训练，它能够提高多模态或多焦点图像中曝光不足/过度曝光区域的融合性能。因此，通过收集多个任务的优势，统一模型可以比多个单独训练的模型获得更好的结果，并且具有更强的泛化能力。

为了解决这些问题，我们提出了一个统一的无监督图像融合网络，称为U2Fusion。对于信息保存，首先采用特征提取器从源图像中提取丰富而全面的特征。然后，测量特征中信息的丰富性以定义这些特征的相对重要性，从而确定源图像与融合结果之间的相似性关系。更高的相似性要求在结果中保留该源图像中的更多信息，从而导致更高的信息保存程度。在这些策略的基础上，对DenseNet模块进行了训练，以生成融合结果，而无需地面真相。我们工作的特点和贡献总结如下:

*  我们为各种图像融合任务提出了一个统一的框架。更具体地说，我们用统一的模型和统一的参数来解决不同的融合问题。我们的解决方案缓解了一些缺点，例如针对不同问题的单独解决方案、用于训练的存储和计算问题以及用于持续学习的灾难性遗忘。 
* 我们通过约束融合图像和源图像之间的相似性来开发一种新的图像融合无监督网络，以克服大多数图像融合问题中的普遍绊脚石，即缺乏普遍的地面实况和无参考度量。
*  我们在六个数据集上测试了所提出的方法，用于多模态、多曝光和多焦点图像融合。定性和定量结果验证了 U2Fusion 的有效性和普遍性。 

 本文的初步版本出现在 [11] 中。新的贡献主要来自四个方面。首先，改进了信息保存度分配策略。不是原始源图像中信息的数量和质量，而是通过对提取的特征执行的信息测量来分配信息保存程度。通过考虑其他方面，修改后的策略提供了改进的综合测量，以捕获源图像的基本特征。其次，修改了损失函数。梯度损失的去除减轻了错误边缘，增加的基于像素强度的损失有助于减少融合图像中的亮度偏差。第三，我们将第一个任务从可见 (VIS) 和红外 (IR) 图像融合替换为多模态图像融合，其中包括 VIS-IR 和医学图像融合。最后，我们在其他公开可用的数据集上验证 U2Fusion。对于消融研究，为了验证弹性权重合并（EWC）对于从新任务中持续学习的有效性 [12]，我们从两个额外的方面分析 EWC，即 EWC 的权重统计分布和中间结果训练阶段的所有任务。对于自适应信息保存度，还对其有效性进行了验证。 

### 二.相关工作

#### 2.1图像融合方法

* 传统图像融合

  *  传统的融合框架可以大致概括为图2。由于重构通常是提取的逆过程，因此这些算法的关键在于两个重要因素: **特征提取和特征融合**。通过修改它们，这些方法可以设计用于解决多模态，多曝光或多焦点图像融合。

  * 为了解决特征提取问题，已经提出了大量的传统方法。它们所基于的理论可以分为四个代表性的类别: 

    * i) 多尺度变换，例如拉普拉斯金字塔 (LP)，低通金字塔比率 (RP)，梯度金字塔 (GP)，离散小波 (DWT)，离散余弦 (DCT) ，curvelet变换 (CVT)，shearlet等; 
    * ii) 稀疏表示 ; 
    * iii) 子空间分析，例如独立分量分析 (ICA)，主成分分析 (PCA)，非负矩阵分解 (NMF) 等；
    * iv) 混合方法。这些人工设计的提取方法使融合方法变得越来越复杂，从而加剧了设计融合规则的难度。为了解决不同的融合任务，需要对提取方法进行相应的修改。此外，需要特别注意提取方法的适当性，以确保特征的完整性。为了克服这些限制，一些方法在特征提取中引入卷积神经网络 (CNN)，要么作为一些子部分 ，要么作为整个部分 。 

    ![1666873688723](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1666873688723.png)

<center textsize=8px>传统融合算法</center>

*  端到端模型 
  *  为了避免设计融合规则，已经提出了许多基于深度学习的算法。与传统中的方法不同，这些方法通常是针对特定融合任务量身定制的端到端模型。 
  * **多模态图像融合：**用于多模态图像融合的端到端模型通常设计用于VIS和IR图像融合。通过在生成器和鉴别器之间建立对抗博弈来保留IR马ge中的像素强度分布和VIS马ge中的细节，提出了FusionGAN 。后来，它的变体被提出通过引入目标增强损失来锐化热目标的边缘。DDcGAN 通过引入双重鉴别器体系结构来增强热目标的突出性。但是，==VIS和IR图像融合中的独特问题是保留像素强度分布和细节，这不适用于其他融合任务。==此外，地面真相通常不存在于这类任务中。因此，这是在多模态图像融合中利用监督学习的主要障碍。
  *  **多曝光图像融合：**为了解决这个问题，提出了一些无监督的方法。Prabhakar等人提出Deepfuse ，其中采用无参考度量mef-ssim作为损失函数。但是，MEFSSIM通过丢弃亮度分量而特别设计用于多曝光图像，因为它在该问题中并不重要。尽管如此，它在其他任务中仍然发挥着重要作用。因此，mef-ssim不适用于其他问题。在一些多曝光数据集中，监督学习没有数据标签。 
  * **多焦点图像融合：**对于这个问题，Liu等人提出了一个网络来生成焦点图 。预定义的标签 (指示它们是高质量图像还是高斯模糊图像) 用于监督学习。然后，它被扩展到一个通用的图像融合框架 。根据泛化情况，可以使用在多焦点图像融合上训练的模型来解决其他任务。此外，郭等人提出了FuseGAN ，其中生成器直接产生一个二进制聚焦蒙版，鉴别器试图将生成的蒙版与地面真相区分开，地面真相是通过利用归一化的磁盘点扩展函数并分离背景和前景来合成的。聚焦图/遮罩对于多焦点图像融合很重要，而它们在其他任务中不是必需的，甚至不适用。所有这些方法都是基于监督学习。
  * **我们的方法：**考虑到上述限制，我们提出了一个统一的无监督图像融合网络，具有以下特点。
    *  i) 它是一种不受人工设计的融合规则限制的端到端模型。 
    * ii) 它是各种融合任务的统一模型，而不是特定目标，例如，独特的问题、度量的特殊性、二进制掩码的需要等。
    *  iii) 它是一个无监督模型，不需要基本事实。
    *  iv) 通过不断学习解决新任务而不丢失旧能力，它以统一的参数解决了多个任务。
* 持续学习
  *  在持续的学习环境中，学习被视为要学习的任务序列。在训练阶段，权重会适应新任务，而不会忘记以前学习的权重。为了避免存储来自先前学习的任务的任何训练数据，提出了许多基于弹性权重合并 (EWC) 的算法 [28]，[29]，其中包括一个正则化项，以迫使参数保持接近那些为先前任务训练的算法。这些技术已广泛应用于许多实际问题中，例如人的重新识别 [30]，实时车辆检测 [31] 和情绪识别 [32] 等。在这项研究中，我们进行持续学习以解决多个融合任务。 

### 三.方法

#### 3.1问题表述

专注于图像融合的主要目标，即保留源图像中的重要信息，我们的模型基于测量来确定此类信息的丰富性。如果源图像包含丰富的信息，则对融合结果非常重要，应该与源图像具有较高的相似度。因此，我们方法的关键问题是探索统一的度量来确定源图像的信息保存程度。我们的方法不是在监督学习中最大化融合结果和基本事实之间的相似性，而是依靠这种程度来保持与源图像的自适应相似性。而且，作为一种无监督模型，它适用于基本事实几乎不可用的多个融合问题。

对于期望的测量，一个主要问题是不同类型的源图像中的生命信息差异很大。例如，在 IR 和正电子发射断层扫描 (PET) 图像中，重要信息是热辐射和功能响应，它们以像素强度分布的形式呈现。在 VIS 和磁共振成像 (MRI) 图像中，重要信息是由图像梯度 [19]、[23] 表示的反射光和结构内容。在多焦点图像中，要保留的信息包括景深（DoF）内的对象。在多曝光图像中，可以增强与场景内容有关的重要信息。上述可变性给设计统一的信息度量带来了相当大的困难，为特定任务设计的统一信息度量在面临其他问题时不再有效。它们在不同的任务中基于某些表层特征或特定属性，很难以统一的方式预先确定。我们通过综合考虑源图像的多方面属性来解决这个问题。为此，我们提取浅层特征（纹理、局部形状等）和深层特征（内容、空间结构等）来估计信息测量。

 U2Fusion的流水线总结为下图。使用表示为I1和I2的源图像，对DenseNet进行训练以生成融合图像If。特征提取的输出是特征图 φ c1 (I1),....,φ c5(I1)和φ c1(I2) ，....， φ c5(I2)。然后对这些特征图执行信息测量，产生由gI1和gI2表示的两个测量。在后续处理中，最终信息保存度表示为 ω1和 ω2。I1，I2，If，ω1和 ω2用于损失函数中，而无需地面真实。在训练阶段，测量 ω1和 ω2并将其应用于定义损失函数。然后，对DenseNet模块进行优化，以最小化损失函数。在测试阶段，不需要测量 ω1和 ω2，因为DenseNet已经过优化。详细的定义或描述在以下小节中给出。

![1666877946156](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1666877946156.png) 

#### 3.2特征提取

与在融合任务中训练的模型相比，其他计算机视觉任务的模型通常使用更大，更多样化的数据集进行训练。因此，这种模型提取的特征是丰富和全面的。受感知损失的启发，我们采用预训练的VGG-16网络 进行特征提取，如图4所示。输入I在我们的模型中已统一在单个通道中 (我们将在第3.5节中讨论此转换)，我们把它复制成三个通道，然后把它们输入VGG-16。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1666879675498.png" alt="1666879675498" style="zoom:80%;" />

在最大池化层之前的卷积层的输出是用于后续信息测量的特征图，其在图4中示为 φC1 (I)，....，φC5 (I)，其形状如下所示。为了直观分析，多曝光图像对的一些特征图如下图所示。在原始源图像中，曝光过度的图像比曝光不足的图像包含更多的纹理细节或更大的梯度，因为后者的亮度低得多。在图5中，φC1 (I) 和 φC2 (I) 中的特征基于浅特征，例如纹理和形状细节。在这些层中，曝光过度的图像的特征图仍然比曝光不足的图像显示更多的信息。相比之下，较高层的特征图 (例如 φC4 (I) 和 φC5(I)) 主要保留了深层特征，例如内容或空间结构。在这些层中，曝光不足图像的特征图中存在可比较的和附加的信息。因此，浅层和深层特征的结合形成了人类视觉感知系统可能不容易感知的基本信息的综合表示。

#### 3.3信息测量

为了测量提取的特征图中包含的信息，它们的梯度用于评估。与从一般信息论导出的实体相比，图像梯度是基于具有小感受野的局部空间结构的度量。在深度学习框架中使用时，梯度在计算和存储方面都更加有效。因此，它们更适合在CNN中用于信息测量。信息度量定义如下:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1666922255955.png" alt="1666922255955" style="zoom:80%;" />

其中 φ cj (I) 是图中第j个最大池化层之前的卷积层的特征图。k表示Dj通道第k个通道中的特征图。||F表示Frobenius范数，而 ▽是拉普拉斯算子。

#### 3.4信息保存度

为了保留源图像中的信息，分配了两个自适应权重作为信息保留度，它们定义了融合图像和源图像之间相似性的权重。权值越高，期望相似度越高，对应的源图像的信息保存程度越高。这些自适应权重 (表示为 ω1和 ω2) 是根据由公式获得的信息测量结果gI1和gI2来估计的。(1)。鉴于gI1和gI2之间的差异是绝对值而不是相对值，因此与它们自身相比可能太小而无法反映它们的差异。因此，为了增强和体现权重的差异，使用预定义的正常数c来缩放值以获得更好的权重分配。因此，ω1和 ω2定义为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1666923586283.png" alt="1666923586283" style="zoom:80%;" />

其中我们使用 softmax 函数将 gI1/c , gI2 /c (2) 映射到 0 和 1 之间的实数，并保证 ω1 和 ω2 之和为 1。然后，在损失函数中使用 ω1 和 ω2 来控制特定源图像的信息保存程度。

#### 3.5损失函数

损失函数主要用于保存重要信息和训练单个模型，适用于多个任务。它由如下定义的两部分组成:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1666924418908.png" alt="1666924418908" style="zoom:80%;" />

其中 θ 表示DenseNet中的参数，D是训练数据集。Lsim(θ，D) 是结果和源图像之间的相似性损失。Lewc(θ，D) 是为持续学习而设计的项目，如下一小节所述。Λ 是控制权衡的超参数。

 我们从结构相似性和强度分布两个方面来实现相似性约束。鉴于结构相似性指数度量 (SSIM) 是根据光，对比度和结构信息的相似性对失真进行建模的最广泛使用的度量 [38]，我们使用它来约束I1，I2和If之间的结构相似性。因此，以 ω1和 ω2来控制信息度，Lsim(θ，D) 的第一项被表述为:

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1666925377108.png" alt="1666925377108" style="zoom:80%;" />

其中Sx，y表示两个图像之间的SSIM值。

它对强度分布的差异显示出较弱的约束。我们用第二项来补充Lssim(θ，D)，该项由两个图像之间的均方误差 (MSE) 定义: